[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "My Blogs",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nLebin Sun\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nLebin Sun\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nLebin Sun\n\n\nInvalid Date\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lebin Sun",
    "section": "",
    "text": "Welcome to my website!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blogs/hw2/index.html",
    "href": "blogs/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nNow, let’s first read in the Blueprinty data.\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the data\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Quick look at the data\ndesc = blueprinty.drop(columns=[\"iscustomer\"]).describe()\ndesc_rounded = desc.copy()\ndesc_rounded.loc[\"mean\"] = desc_rounded.loc[\"mean\"].round(2)\ndesc_rounded.loc[\"std\"] = desc_rounded.loc[\"std\"].round(2)\ndesc_rounded.loc[~desc_rounded.index.isin([\"mean\", \"std\"])] = desc_rounded.loc[~desc_rounded.index.isin([\"mean\", \"std\"])].round(0)\ndesc_rounded.style\\\n    .format(\"{:.2f}\", subset=pd.IndexSlice[\"mean\", :])\\\n    .format(\"{:.2f}\", subset=pd.IndexSlice[\"std\", :])\\\n    .format(\"{:.0f}\", subset=pd.IndexSlice[[\"count\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"], :])\n\n\n\n\n\n\n\n \npatents\nage\n\n\n\n\ncount\n1500\n1500\n\n\nmean\n3.68\n26.36\n\n\nstd\n2.35\n7.24\n\n\nmin\n0\n9\n\n\n25%\n2\n21\n\n\n50%\n3\n26\n\n\n75%\n5\n32\n\n\nmax\n16\n49\n\n\n\n\n\nThe above table displays the statistic summary of numerical variables in the dataset.\nNext, a histogram of number of patens distribution by customer status bellow compares differences between customer groups.\n\n\nShow code\n# Set up the plot style\nsns.set(style=\"whitegrid\")\nmeans = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\npalette = {0: \"orange\", 1: \"green\"}\n# Histogram of number of patents by customer status\nplt.figure(figsize=(8, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", binwidth=1, palette=palette)\n\nplt.axvline(means[0], color=palette[0], linestyle=\"--\", linewidth=2, label=\"Mean (Non-customer)\")\nplt.axvline(means[1], color=palette[1], linestyle=\"--\", linewidth=2, label=\"Mean (Customer)\")\n\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom the histogram and mean values, it appears that customers of Blueprinty’s software tend to have more patents on average than non-customers. This may suggest that the software is associated with greater patenting success, but further modeling is necessary to control for confounding variables.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nAfter observing that Blueprinty customers might not be randomly selected, we explore whether there are systematic differences in region and firm age between customers and non-customers.\n\n\nShow code\n# Bar plot of region distribution by customer status\nblueprinty[\"iscustomer\"] = blueprinty[\"iscustomer\"].astype(str).astype(int)\nplt.figure(figsize=(8, 4))\nsns.countplot(data=blueprinty, x=\"region\", hue=\"iscustomer\", palette={0: \"orange\", 1: \"green\"})\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows that customer adoption varies by region. For instance, the Northeast has a disproportionately high number of customers, while the Southwest and Northwest are dominated by non-customers. This implies that regional variation could confound any relationship between software usage and patenting success, so it should be accounted for in modeling.\n\n\nShow code\n# Boxplot of age by customer status\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=blueprinty, x=\"iscustomer\", y=\"age\", hue=\"iscustomer\", palette={0: \"orange\", 1: \"green\"})\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status (0 = Non-customer, 1 = Customer)\")\nplt.ylabel(\"Age\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCustomers appear to be slightly older than non-customers, with a higher median and a broader age distribution. While the difference is modest, it still points to the possibility that more mature firms are more likely to adopt Blueprinty’s tools. Again, this reinforces the need to adjust for age when estimating treatment effects.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function of a Poisson distribution is:\n\\[\nP(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood for a sample of ( n ) observations is:\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood function is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nNow translate the log-likelihood expression into Python code. The function below takes a proposed value of λ and a vector of observed patent counts Y, and returns the total log-likelihood. We use gammaln(Y + 1) instead of log(Y!) to ensure numerical stability and avoid issues with large factorials.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"Compute the Poisson log-likelihood for a given lambda and data Y\"\"\"\n    if lambda_ &lt;= 0:\n        return -np.inf\n    loglik = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return loglik\n\n\nThis function will allow us to evaluate the fit of any given λ to the data. In the next step, we’ll search for the value of λ that maximizes this log-likelihood — our Maximum Likelihood Estimate (MLE).\n\n\nShow code\n# Define a range of lambda values to evaluate\nlambda_vals = np.linspace(0.1, 10, 200)\n\n# Compute log-likelihood for each lambda value\nloglik_vals = [poisson_loglikelihood(l, blueprinty[\"patents\"]) for l in lambda_vals]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color=\"purple\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood over a Range of λ\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s now find the Maximum Likelihood Estimate (MLE) for lambda by taking the derivative of the log-likelihood and setting it equal to zero:\n[ L() = {i=1}^{n} ( -1 + ) = -n + {i=1}^{n} Y_i ]\nSetting this equal to zero:\n[ -n + Y_i = 0 = Y_i = {Y} ]\nSo the MLE for lambda is simply the sample mean of the observed counts.\nAnd here’s a small Python code block to verify this numerically:\n\n\nShow code\nlambda_mle = blueprinty[\"patents\"].mean()\nlambda_mle.round(3)\n\n\n3.685\n\n\nWe can now find the Maximum Likelihood Estimate (MLE) for lambda numerically by maximizing the log-likelihood function. Since scipy.optimize performs minimization, we minimize the negative log-likelihood over a reasonable range of values for lambda.\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, blueprinty[\"patents\"]),\n    bounds=(0.1, 10),\n    method=\"bounded\"\n)\n\nlambda_mle_opt = result.x\nlambda_mle_opt.round(3)\n\n\n3.685\n\n\nThe numerical estimate of lambda (MLE) is 3.685, which matches the sample mean of the observed patent counts.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    loglik = np.sum(-lambda_ + Y * np.log(lambda_ + 1e-10) - gammaln(Y + 1))\n    return loglik\n\n\nWe prepare the covariate matrix X so that it has the structure needed for Poisson regression: a constant column, numeric variables, and encoded categorical variables.\n\n\nShow code\nimport pandas as pd\nimport numpy as np\n\n# Create dummy variables for region (drop one to avoid multicollinearity)\nblueprinty_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\n# Add age, age^2, customer, and intercept\nblueprinty[\"age2\"] = blueprinty[\"age\"] ** 2\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age2\", \"iscustomer\"]],\n    blueprinty_dummies\n], axis=1)\n\n# Convert to numpy array\nX_mat = X.values.astype(float)\nY = blueprinty[\"patents\"].values.astype(float)\n\n\nWe use scipy.optimize.minimize with the BFGS method to find the MLEs of our regression coefficients.\n\n\nShow code\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood function\nneg_loglik = lambda beta: -poisson_regression_loglikelihood(beta, Y, X_mat)\n\nbeta_init = np.full(X_mat.shape[1], -0.1)\n\n# Minimize it\nresult = minimize(neg_loglik, beta_init, method=\"BFGS\")\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n\nThe table below reports the estimated effect of each variable on the number of patents a firm receives, along with the standard error of each estimate.\n\n\nShow code\n# Standard errors from the inverse Hessian\nse = np.sqrt(np.diag(hessian_inv))\n# Combine into a tidy table\ncoef_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se\n})\ncoef_table.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n\n\n\n\n0\nintercept\n5.7853\n1.0\n\n\n1\nage\n77.2949\n1.0\n\n\n2\nage2\n1031.2168\n1.0\n\n\n3\niscustomer\n2.0352\n1.0\n\n\n4\nNortheast\n2.5411\n1.0\n\n\n5\nNorthwest\n0.6923\n1.0\n\n\n6\nSouth\n0.4914\n1.0\n\n\n7\nSouthwest\n0.9087\n1.0\n\n\n\n\n\n\n\nTo confirm the accuracy of our hand-coded Poisson regression, we also estimate the model using statsmodels.GLM with the Poisson family. The table below presents the coefficient estimates, standard errors, z-scores, and confidence intervals.\n\n\nShow code\nimport statsmodels.api as sm\n\n# Fit the Poisson regression using GLM\nglm_model = sm.GLM(Y, X_mat, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display results in a clean table\nglm_summary = glm_results.summary2().tables[1].reset_index().rename(columns={\"index\": \"Variable\"})\nglm_summary.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nx1\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n2\nx2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n3\nx3\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n4\nx4\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n5\nx5\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n6\nx6\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n7\nx7\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\n\n\n\n\nThe table above summarizes the results from our Poisson regression model using statsmodels. Here’s what we observe:\n\nx1 (age): The coefficient is positive and highly significant (p&lt;0.001), indicating that older firms tend to receive more patents, all else equal.\nx2 (age squared): The coefficient is small, negative, and also highly significant. This suggests a concave relationship — the positive effect of age diminishes at higher ages, meaning the relationship between age and patents is likely hump-shaped.\nx3 (iscustomer): The customer indicator is positive and significant. This implies that firms who use Blueprinty’s software are associated with higher patent counts, controlling for other factors. This is consistent with the hypothesis that the software may help firms better navigate the patent process.\nx4 to x7 (region dummies): enderNone of the region dummy variables are statistically significant at conventional levels (p&gt;0.05). This suggests that after controlling for age and customer status, the region of a firm does not meaningfully affect patent count in this sample.\nIntercept (const): The baseline level of patenting (for a non-customer firm with age zero in the reference region) is negative, as expected — but not of direct substantive interest.\n\nDue to the nonlinear nature of the Poisson regression model, the estimated coefficients—particularly for binary variables like iscustomer—are not directly interpretable in terms of marginal effects on the outcome variable. That is, the coefficient on iscustomer does not represent a constant additive change in the expected number of patents.\nTo obtain a more interpretable estimate of the effect of using Blueprinty’s software, we implement a counterfactual prediction approach. Specifically, we construct two hypothetical datasets:\n\nScenario 1 (X₀): Each firm is treated as a non-customer (iscustomer = 0)\nScenario 2 (X₁): Each firm is treated as a customer (iscustomer = 1)\n\nAll other covariates are held constant. We then use our fitted Poisson model to generate predicted patent counts for each firm under both scenarios. The average difference between the predicted outcomes under X₁ and X₀ represents the estimated effect of Blueprinty’s software on patenting activity.\nBased on this approach, we estimate that firms using Blueprinty’s software produce, on average, 0.79 more patents than they would have without the software, holding all else equal. This provides evidence consistent with the hypothesis that the software enhances patenting success."
  },
  {
    "objectID": "blogs/hw2/index.html#blueprinty-case-study",
    "href": "blogs/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nNow, let’s first read in the Blueprinty data.\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the data\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Quick look at the data\ndesc = blueprinty.drop(columns=[\"iscustomer\"]).describe()\ndesc_rounded = desc.copy()\ndesc_rounded.loc[\"mean\"] = desc_rounded.loc[\"mean\"].round(2)\ndesc_rounded.loc[\"std\"] = desc_rounded.loc[\"std\"].round(2)\ndesc_rounded.loc[~desc_rounded.index.isin([\"mean\", \"std\"])] = desc_rounded.loc[~desc_rounded.index.isin([\"mean\", \"std\"])].round(0)\ndesc_rounded.style\\\n    .format(\"{:.2f}\", subset=pd.IndexSlice[\"mean\", :])\\\n    .format(\"{:.2f}\", subset=pd.IndexSlice[\"std\", :])\\\n    .format(\"{:.0f}\", subset=pd.IndexSlice[[\"count\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"], :])\n\n\n\n\n\n\n\n \npatents\nage\n\n\n\n\ncount\n1500\n1500\n\n\nmean\n3.68\n26.36\n\n\nstd\n2.35\n7.24\n\n\nmin\n0\n9\n\n\n25%\n2\n21\n\n\n50%\n3\n26\n\n\n75%\n5\n32\n\n\nmax\n16\n49\n\n\n\n\n\nThe above table displays the statistic summary of numerical variables in the dataset.\nNext, a histogram of number of patens distribution by customer status bellow compares differences between customer groups.\n\n\nShow code\n# Set up the plot style\nsns.set(style=\"whitegrid\")\nmeans = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\npalette = {0: \"orange\", 1: \"green\"}\n# Histogram of number of patents by customer status\nplt.figure(figsize=(8, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", binwidth=1, palette=palette)\n\nplt.axvline(means[0], color=palette[0], linestyle=\"--\", linewidth=2, label=\"Mean (Non-customer)\")\nplt.axvline(means[1], color=palette[1], linestyle=\"--\", linewidth=2, label=\"Mean (Customer)\")\n\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom the histogram and mean values, it appears that customers of Blueprinty’s software tend to have more patents on average than non-customers. This may suggest that the software is associated with greater patenting success, but further modeling is necessary to control for confounding variables.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nAfter observing that Blueprinty customers might not be randomly selected, we explore whether there are systematic differences in region and firm age between customers and non-customers.\n\n\nShow code\n# Bar plot of region distribution by customer status\nblueprinty[\"iscustomer\"] = blueprinty[\"iscustomer\"].astype(str).astype(int)\nplt.figure(figsize=(8, 4))\nsns.countplot(data=blueprinty, x=\"region\", hue=\"iscustomer\", palette={0: \"orange\", 1: \"green\"})\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows that customer adoption varies by region. For instance, the Northeast has a disproportionately high number of customers, while the Southwest and Northwest are dominated by non-customers. This implies that regional variation could confound any relationship between software usage and patenting success, so it should be accounted for in modeling.\n\n\nShow code\n# Boxplot of age by customer status\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=blueprinty, x=\"iscustomer\", y=\"age\", hue=\"iscustomer\", palette={0: \"orange\", 1: \"green\"})\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status (0 = Non-customer, 1 = Customer)\")\nplt.ylabel(\"Age\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCustomers appear to be slightly older than non-customers, with a higher median and a broader age distribution. While the difference is modest, it still points to the possibility that more mature firms are more likely to adopt Blueprinty’s tools. Again, this reinforces the need to adjust for age when estimating treatment effects.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function of a Poisson distribution is:\n\\[\nP(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood for a sample of ( n ) observations is:\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood function is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nNow translate the log-likelihood expression into Python code. The function below takes a proposed value of λ and a vector of observed patent counts Y, and returns the total log-likelihood. We use gammaln(Y + 1) instead of log(Y!) to ensure numerical stability and avoid issues with large factorials.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"Compute the Poisson log-likelihood for a given lambda and data Y\"\"\"\n    if lambda_ &lt;= 0:\n        return -np.inf\n    loglik = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return loglik\n\n\nThis function will allow us to evaluate the fit of any given λ to the data. In the next step, we’ll search for the value of λ that maximizes this log-likelihood — our Maximum Likelihood Estimate (MLE).\n\n\nShow code\n# Define a range of lambda values to evaluate\nlambda_vals = np.linspace(0.1, 10, 200)\n\n# Compute log-likelihood for each lambda value\nloglik_vals = [poisson_loglikelihood(l, blueprinty[\"patents\"]) for l in lambda_vals]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color=\"purple\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood over a Range of λ\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s now find the Maximum Likelihood Estimate (MLE) for lambda by taking the derivative of the log-likelihood and setting it equal to zero:\n[ L() = {i=1}^{n} ( -1 + ) = -n + {i=1}^{n} Y_i ]\nSetting this equal to zero:\n[ -n + Y_i = 0 = Y_i = {Y} ]\nSo the MLE for lambda is simply the sample mean of the observed counts.\nAnd here’s a small Python code block to verify this numerically:\n\n\nShow code\nlambda_mle = blueprinty[\"patents\"].mean()\nlambda_mle.round(3)\n\n\n3.685\n\n\nWe can now find the Maximum Likelihood Estimate (MLE) for lambda numerically by maximizing the log-likelihood function. Since scipy.optimize performs minimization, we minimize the negative log-likelihood over a reasonable range of values for lambda.\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, blueprinty[\"patents\"]),\n    bounds=(0.1, 10),\n    method=\"bounded\"\n)\n\nlambda_mle_opt = result.x\nlambda_mle_opt.round(3)\n\n\n3.685\n\n\nThe numerical estimate of lambda (MLE) is 3.685, which matches the sample mean of the observed patent counts.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    loglik = np.sum(-lambda_ + Y * np.log(lambda_ + 1e-10) - gammaln(Y + 1))\n    return loglik\n\n\nWe prepare the covariate matrix X so that it has the structure needed for Poisson regression: a constant column, numeric variables, and encoded categorical variables.\n\n\nShow code\nimport pandas as pd\nimport numpy as np\n\n# Create dummy variables for region (drop one to avoid multicollinearity)\nblueprinty_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\n# Add age, age^2, customer, and intercept\nblueprinty[\"age2\"] = blueprinty[\"age\"] ** 2\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age2\", \"iscustomer\"]],\n    blueprinty_dummies\n], axis=1)\n\n# Convert to numpy array\nX_mat = X.values.astype(float)\nY = blueprinty[\"patents\"].values.astype(float)\n\n\nWe use scipy.optimize.minimize with the BFGS method to find the MLEs of our regression coefficients.\n\n\nShow code\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood function\nneg_loglik = lambda beta: -poisson_regression_loglikelihood(beta, Y, X_mat)\n\nbeta_init = np.full(X_mat.shape[1], -0.1)\n\n# Minimize it\nresult = minimize(neg_loglik, beta_init, method=\"BFGS\")\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n\nThe table below reports the estimated effect of each variable on the number of patents a firm receives, along with the standard error of each estimate.\n\n\nShow code\n# Standard errors from the inverse Hessian\nse = np.sqrt(np.diag(hessian_inv))\n# Combine into a tidy table\ncoef_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se\n})\ncoef_table.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n\n\n\n\n0\nintercept\n5.7853\n1.0\n\n\n1\nage\n77.2949\n1.0\n\n\n2\nage2\n1031.2168\n1.0\n\n\n3\niscustomer\n2.0352\n1.0\n\n\n4\nNortheast\n2.5411\n1.0\n\n\n5\nNorthwest\n0.6923\n1.0\n\n\n6\nSouth\n0.4914\n1.0\n\n\n7\nSouthwest\n0.9087\n1.0\n\n\n\n\n\n\n\nTo confirm the accuracy of our hand-coded Poisson regression, we also estimate the model using statsmodels.GLM with the Poisson family. The table below presents the coefficient estimates, standard errors, z-scores, and confidence intervals.\n\n\nShow code\nimport statsmodels.api as sm\n\n# Fit the Poisson regression using GLM\nglm_model = sm.GLM(Y, X_mat, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display results in a clean table\nglm_summary = glm_results.summary2().tables[1].reset_index().rename(columns={\"index\": \"Variable\"})\nglm_summary.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nx1\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n2\nx2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n3\nx3\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n4\nx4\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n5\nx5\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n6\nx6\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n7\nx7\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\n\n\n\n\nThe table above summarizes the results from our Poisson regression model using statsmodels. Here’s what we observe:\n\nx1 (age): The coefficient is positive and highly significant (p&lt;0.001), indicating that older firms tend to receive more patents, all else equal.\nx2 (age squared): The coefficient is small, negative, and also highly significant. This suggests a concave relationship — the positive effect of age diminishes at higher ages, meaning the relationship between age and patents is likely hump-shaped.\nx3 (iscustomer): The customer indicator is positive and significant. This implies that firms who use Blueprinty’s software are associated with higher patent counts, controlling for other factors. This is consistent with the hypothesis that the software may help firms better navigate the patent process.\nx4 to x7 (region dummies): enderNone of the region dummy variables are statistically significant at conventional levels (p&gt;0.05). This suggests that after controlling for age and customer status, the region of a firm does not meaningfully affect patent count in this sample.\nIntercept (const): The baseline level of patenting (for a non-customer firm with age zero in the reference region) is negative, as expected — but not of direct substantive interest.\n\nDue to the nonlinear nature of the Poisson regression model, the estimated coefficients—particularly for binary variables like iscustomer—are not directly interpretable in terms of marginal effects on the outcome variable. That is, the coefficient on iscustomer does not represent a constant additive change in the expected number of patents.\nTo obtain a more interpretable estimate of the effect of using Blueprinty’s software, we implement a counterfactual prediction approach. Specifically, we construct two hypothetical datasets:\n\nScenario 1 (X₀): Each firm is treated as a non-customer (iscustomer = 0)\nScenario 2 (X₁): Each firm is treated as a customer (iscustomer = 1)\n\nAll other covariates are held constant. We then use our fitted Poisson model to generate predicted patent counts for each firm under both scenarios. The average difference between the predicted outcomes under X₁ and X₀ represents the estimated effect of Blueprinty’s software on patenting activity.\nBased on this approach, we estimate that firms using Blueprinty’s software produce, on average, 0.79 more patents than they would have without the software, holding all else equal. This provides evidence consistent with the hypothesis that the software enhances patenting success."
  },
  {
    "objectID": "blogs/hw2/index.html#airbnb-case-study",
    "href": "blogs/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\nLoad and Clean Data\nWe begin by loading the AirBnB dataset, which is shown as below. The goal is to understand what factors are associated with a higher number of reviews, which we treat as a proxy for booking activity.\n\n\nShow code\nimport pandas as pd\n\n# Load the dataset\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n\nBelow is a summary of the dataset’s structure and variable distributions.\n\n\nShow code\nairbnb.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nUnnamed: 0\n40628.0\n2.031450e+04\n1.172844e+04\n1.0\n10157.75\n20314.5\n30471.25\n40628.0\n\n\nid\n40628.0\n9.698889e+06\n5.460166e+06\n2515.0\n4889868.25\n9862877.5\n14667893.75\n18009669.0\n\n\ndays\n40628.0\n1.102368e+03\n1.383269e+03\n1.0\n542.00\n996.0\n1535.00\n42828.0\n\n\nbathrooms\n40468.0\n1.124592e+00\n3.858843e-01\n0.0\n1.00\n1.0\n1.00\n8.0\n\n\nbedrooms\n40552.0\n1.147046e+00\n6.917461e-01\n0.0\n1.00\n1.0\n1.00\n10.0\n\n\nprice\n40628.0\n1.447607e+02\n2.106576e+02\n10.0\n70.00\n100.0\n170.00\n10000.0\n\n\nnumber_of_reviews\n40628.0\n1.590443e+01\n2.924601e+01\n0.0\n1.00\n4.0\n17.00\n421.0\n\n\nreview_scores_cleanliness\n30433.0\n9.198370e+00\n1.119935e+00\n2.0\n9.00\n10.0\n10.00\n10.0\n\n\nreview_scores_location\n30374.0\n9.413544e+00\n8.449491e-01\n2.0\n9.00\n10.0\n10.00\n10.0\n\n\nreview_scores_value\n30372.0\n9.331522e+00\n9.029656e-01\n2.0\n9.00\n10.0\n10.00\n10.0\n\n\n\n\n\n\n\nSeveral variables in the dataset contain missing values. Notably, over 10,000 listings are missing review scores related to value, location, and cleanliness—likely because those listings received no reviews. Since our outcome of interest is the number of reviews, we can reasonably restrict our analysis to listings with at least one review. Additionally, we drop a small number of rows with missing data in key listing features such as bathrooms, bedrooms, and host listing date. Here’s the code to filter and clean accordingly:\n\n\nShow code\n# Drop listings with zero reviews\nairbnb_filtered = airbnb[airbnb[\"number_of_reviews\"] &gt; 0].copy()\n\n# Drop rows with missing values in relevant predictors\ncols_to_check = [\"bathrooms\", \"bedrooms\", \"host_since\",\n                 \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]\nairbnb_filtered = airbnb_filtered.dropna(subset=cols_to_check)\n\n\n\n\nExploratory Data Analysis\nTo begin, we examine the distributions of key variables:\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\nsns.histplot(airbnb_filtered[\"number_of_reviews\"], bins=50, color=\"green\", kde=False, ax=axes[0, 0])\naxes[0, 0].set_title(\"Number of Reviews\")\n\nsns.histplot(airbnb_filtered[\"price\"], bins=50, color=\"green\", kde=False, ax=axes[0, 1])\naxes[0, 1].set_title(\"Price per Night\")\n\nsns.histplot(airbnb_filtered[\"review_scores_cleanliness\"], bins=10, color=\"green\", kde=False, ax=axes[1, 0])\naxes[1, 0].set_title(\"Cleanliness Score\")\n\nsns.histplot(airbnb_filtered[\"review_scores_value\"], bins=10, color=\"green\", kde=False, ax=axes[1, 1])\naxes[1, 1].set_title(\"Value Score\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNumber of Reviews: Highly right-skewed. Most listings receive a modest number of reviews, but a small number receive over 100. This validates the use of a count-based model such as Poisson regression.\nPrice per Night: Also right-skewed, with the majority of listings priced under $500. A few outliers appear above $1,000.\nReview Scores (Cleanliness & Value): Both are tightly clustered near the upper end of the scale (around 9–10), suggesting generally favorable customer feedback with limited variation.\nThese patterns support the idea that modeling review counts (as a proxy for bookings) using a Poisson regression is appropriate, provided we carefully control for influential predictors like price and review quality.\n\n\nPoisson Regression: Predicting Number of Reviews\nWe fit a Poisson regression model to predict the number of reviews (used as a proxy for bookings) based on a selection of listing characteristics.\n\n\nShow code\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Select predictors\npredictors = [\n    \"price\",\n    \"review_scores_cleanliness\",\n    \"review_scores_location\",\n    \"review_scores_value\",\n    \"bedrooms\",\n    \"bathrooms\"\n]\n\n# Drop rows with missing data\nairbnb_model_data = airbnb_filtered.dropna(subset=predictors + [\"number_of_reviews\"]).copy()\n\n# Create design matrix\nX = sm.add_constant(airbnb_model_data[predictors])\nY = airbnb_model_data[\"number_of_reviews\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Display summary\nsummary_table = poisson_model.summary2().tables[1].reset_index().rename(columns={\"index\": \"Variable\"})\nsummary_table.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n3.7035\n0.0157\n236.5826\n0.0000\n3.6729\n3.7342\n\n\n1\nprice\n-0.0000\n0.0000\n-3.1641\n0.0016\n-0.0000\n-0.0000\n\n\n2\nreview_scores_cleanliness\n0.1138\n0.0015\n76.6885\n0.0000\n0.1109\n0.1167\n\n\n3\nreview_scores_location\n-0.0803\n0.0016\n-50.3224\n0.0000\n-0.0834\n-0.0771\n\n\n4\nreview_scores_value\n-0.0968\n0.0018\n-54.2270\n0.0000\n-0.1003\n-0.0933\n\n\n5\nbedrooms\n0.0770\n0.0020\n38.8908\n0.0000\n0.0731\n0.0809\n\n\n6\nbathrooms\n-0.1179\n0.0038\n-31.3070\n0.0000\n-0.1253\n-0.1106\n\n\n\n\n\n\n\n\nIntercept: Represents the expected log number of reviews when all predictors are at zero.\nPrice: Small, negative, and statistically significant — more expensive listings receive slightly fewer reviews, on average.\nReview Scores – Cleanliness: Positive and highly significant. Cleanliness appears to drive guest engagement.\nReview Scores – Location & Value: Both are negative and significant, which may seem surprising. This could reflect multicollinearity or indicate that higher ratings correspond with fewer, but higher-quality bookings.\n\nAll coefficients are statistically significant at the 0.001 level, suggesting meaningful associations between listing characteristics and review frequency.\n\n\nModel Fit: Predicted vs. Actual Reviews\n\n\nShow code\nairbnb_model_data[\"predicted_reviews\"] = poisson_model.predict(X)\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(\n    x=airbnb_model_data[\"number_of_reviews\"],\n    y=airbnb_model_data[\"predicted_reviews\"],\n    alpha=0.5, \n    color=\"green\"\n)\nplt.xlabel(\"Actual Number of Reviews\")\nplt.ylabel(\"Predicted Number of Reviews\")\nplt.title(\"Predicted vs. Actual Number of Reviews (Poisson Model)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nComparing predicted review counts from the Poisson model to the actual observed counts for each listing, the plot shows a strong positive relationship between actual and predicted values, especially for listings with fewer than 100 reviews. There is some deviation for listings with very high review counts, suggesting possible overdispersion—a common issue in count data where variance exceeds the mean. Despite this, the model captures the general trend well and provides a solid baseline for understanding how listing characteristics relate to booking activity."
  },
  {
    "objectID": "blogs/hw1/index.html",
    "href": "blogs/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis study investigate whether the presence and size of a matching grant—commonly used by nonprofits—significantly affects donor behavior. The matching grant acts as a potential price reduction for donating, which may alter individuals’ willingness to give. The experiment’s large sample and randomized design make it especially well-suited to test economic theories of altruism, incentives, and social signaling.\nThis project seeks to replicate the core findings of Karlan and List (2007), using the original dataset provided by the authors. Through the replication, I aim to confirm their main results, including how matching grants affect both the likelihood of donating and the average amount donated. I also explore potential heterogeneity in treatment effects by political geography, as highlighted in the original paper."
  },
  {
    "objectID": "blogs/hw1/index.html#introduction",
    "href": "blogs/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis study investigate whether the presence and size of a matching grant—commonly used by nonprofits—significantly affects donor behavior. The matching grant acts as a potential price reduction for donating, which may alter individuals’ willingness to give. The experiment’s large sample and randomized design make it especially well-suited to test economic theories of altruism, incentives, and social signaling.\nThis project seeks to replicate the core findings of Karlan and List (2007), using the original dataset provided by the authors. Through the replication, I aim to confirm their main results, including how matching grants affect both the likelihood of donating and the average amount donated. I also explore potential heterogeneity in treatment effects by political geography, as highlighted in the original paper."
  },
  {
    "objectID": "blogs/hw1/index.html#data",
    "href": "blogs/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis dataset comes from a large-scale natural field experiment conducted by Karlan and List (2007) to study the impact of different fundraising strategies on charitable donations. It contains data from over 50,000 past donors to a U.S.-based nonprofit organization who were randomly assigned to receive one of several types of solicitation letters.\nEach observation in the dataset corresponds to a single individual donor. The dataset includes variables that capture:\n- Treatment assignment including whether the donor received a matching grant offer and the specific match ratio (1:1, 2:1, or 3:1).\n- Suggested donation amounts which were manipulated as part of the experimental design.\n- Donation behavior such as whether the person donated in response, and how much they gave.\n- Demographic indicators and prior donation history (e.g., years since first gift, total number of past donations, gender).\n- Geographic and political indicators such as whether the donor lived in a “red” or “blue” state during the 2004 presidential election, and the level of activity of the organization in that state.\nThe following Python code chunk loads the original dataset Dean Karlan used:\n\n\nShow code\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n\n\n\nData Overview\nThis following table provides an overview of all variables in the dataset. It includes each variable’s name, its data type (e.g., numeric or categorical), and the number of missing values.\n\n\nShow code\noverview_table = pd.DataFrame({\n    \"Variable\": df.columns,\n    \"Data Type\": df.dtypes.values,\n    \"Missing Values\": df.isnull().sum().values\n})\n\noverview_table\n\n\n\n\n\n\n\n\n\nVariable\nData Type\nMissing Values\n\n\n\n\n0\ntreatment\nint8\n0\n\n\n1\ncontrol\nint8\n0\n\n\n2\nratio\ncategory\n0\n\n\n3\nratio2\nint8\n0\n\n\n4\nratio3\nint8\n0\n\n\n5\nsize\ncategory\n0\n\n\n6\nsize25\nint8\n0\n\n\n7\nsize50\nint8\n0\n\n\n8\nsize100\nint8\n0\n\n\n9\nsizeno\nint8\n0\n\n\n10\nask\ncategory\n0\n\n\n11\naskd1\nint8\n0\n\n\n12\naskd2\nint8\n0\n\n\n13\naskd3\nint8\n0\n\n\n14\nask1\nint16\n0\n\n\n15\nask2\nint16\n0\n\n\n16\nask3\nint16\n0\n\n\n17\namount\nfloat32\n0\n\n\n18\ngave\nint8\n0\n\n\n19\namountchange\nfloat32\n0\n\n\n20\nhpa\nfloat32\n0\n\n\n21\nltmedmra\nint8\n0\n\n\n22\nfreq\nint16\n0\n\n\n23\nyears\nfloat64\n1\n\n\n24\nyear5\nint8\n0\n\n\n25\nmrm2\nfloat64\n1\n\n\n26\ndormant\nint8\n0\n\n\n27\nfemale\nfloat64\n1111\n\n\n28\ncouple\nfloat64\n1148\n\n\n29\nstate50one\nint8\n0\n\n\n30\nnonlit\nfloat64\n452\n\n\n31\ncases\nfloat64\n452\n\n\n32\nstatecnt\nfloat32\n0\n\n\n33\nstateresponse\nfloat32\n0\n\n\n34\nstateresponset\nfloat32\n0\n\n\n35\nstateresponsec\nfloat32\n3\n\n\n36\nstateresponsetminc\nfloat32\n3\n\n\n37\nperbush\nfloat32\n35\n\n\n38\nclose25\nfloat64\n35\n\n\n39\nred0\nfloat64\n35\n\n\n40\nblue0\nfloat64\n35\n\n\n41\nredcty\nfloat64\n105\n\n\n42\nbluecty\nfloat64\n105\n\n\n43\npwhite\nfloat32\n1866\n\n\n44\npblack\nfloat32\n2036\n\n\n45\npage18_39\nfloat32\n1866\n\n\n46\nave_hh_sz\nfloat32\n1862\n\n\n47\nmedian_hhincome\nfloat64\n1874\n\n\n48\npowner\nfloat32\n1869\n\n\n49\npsch_atlstba\nfloat32\n1868\n\n\n50\npop_propurban\nfloat32\n1866\n\n\n\n\n\n\n\n\n\nVariable Definitions\nThe summary statistics table below presents descriptive metrics for all numeric variables in this dataset, including count, mean, standard deviation, and range (min/max).\n\n\nShow code\ndf.describe().transpose().round(2)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntreatment\n50083.0\n0.67\n0.47\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\ncontrol\n50083.0\n0.33\n0.47\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\nratio2\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nratio3\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nsize25\n50083.0\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nsize50\n50083.0\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nsize100\n50083.0\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nsizeno\n50083.0\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\naskd1\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\naskd2\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\naskd3\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nask1\n50083.0\n71.50\n101.73\n25.00\n35.00\n45.00\n65.00\n1500.00\n\n\nask2\n50083.0\n91.79\n127.25\n35.00\n45.00\n60.00\n85.00\n1875.00\n\n\nask3\n50083.0\n111.05\n151.67\n50.00\n55.00\n70.00\n100.00\n2250.00\n\n\namount\n50083.0\n0.92\n8.71\n0.00\n0.00\n0.00\n0.00\n400.00\n\n\ngave\n50083.0\n0.02\n0.14\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\namountchange\n50083.0\n-52.67\n1267.10\n-200412.12\n-50.00\n-30.00\n-25.00\n275.00\n\n\nhpa\n50083.0\n59.38\n71.18\n0.00\n30.00\n45.00\n60.00\n1000.00\n\n\nltmedmra\n50083.0\n0.49\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\nfreq\n50083.0\n8.04\n11.39\n0.00\n2.00\n4.00\n10.00\n218.00\n\n\nyears\n50082.0\n6.10\n5.50\n0.00\n2.00\n5.00\n9.00\n95.00\n\n\nyear5\n50083.0\n0.51\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\nmrm2\n50082.0\n13.01\n12.08\n0.00\n4.00\n8.00\n19.00\n168.00\n\n\ndormant\n50083.0\n0.52\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\nfemale\n48972.0\n0.28\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\ncouple\n48935.0\n0.09\n0.29\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nstate50one\n50083.0\n0.00\n0.03\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nnonlit\n49631.0\n2.47\n1.96\n0.00\n1.00\n3.00\n4.00\n6.00\n\n\ncases\n49631.0\n1.50\n1.16\n0.00\n1.00\n1.00\n2.00\n4.00\n\n\nstatecnt\n50083.0\n6.00\n5.75\n0.00\n1.83\n3.54\n9.61\n17.37\n\n\nstateresponse\n50083.0\n0.02\n0.01\n0.00\n0.02\n0.02\n0.02\n0.08\n\n\nstateresponset\n50083.0\n0.02\n0.01\n0.00\n0.02\n0.02\n0.02\n0.11\n\n\nstateresponsec\n50080.0\n0.02\n0.01\n0.00\n0.01\n0.02\n0.02\n0.05\n\n\nstateresponsetminc\n50080.0\n0.00\n0.01\n-0.05\n-0.00\n0.00\n0.01\n0.11\n\n\nperbush\n50048.0\n0.49\n0.08\n0.09\n0.44\n0.48\n0.53\n0.73\n\n\nclose25\n50048.0\n0.19\n0.39\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nred0\n50048.0\n0.40\n0.49\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\nblue0\n50048.0\n0.60\n0.49\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\nredcty\n49978.0\n0.51\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\nbluecty\n49978.0\n0.49\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\npwhite\n48217.0\n0.82\n0.17\n0.01\n0.76\n0.87\n0.94\n1.00\n\n\npblack\n48047.0\n0.09\n0.14\n0.00\n0.01\n0.04\n0.09\n0.99\n\n\npage18_39\n48217.0\n0.32\n0.10\n0.00\n0.26\n0.31\n0.37\n1.00\n\n\nave_hh_sz\n48221.0\n2.43\n0.38\n0.00\n2.21\n2.44\n2.66\n5.27\n\n\nmedian_hhincome\n48209.0\n54815.70\n22027.32\n5000.00\n39181.00\n50673.00\n66005.00\n200001.00\n\n\npowner\n48214.0\n0.67\n0.19\n0.00\n0.56\n0.71\n0.82\n1.00\n\n\npsch_atlstba\n48215.0\n0.39\n0.19\n0.00\n0.24\n0.37\n0.53\n1.00\n\n\npop_propurban\n48217.0\n0.87\n0.26\n0.00\n0.88\n1.00\n1.00\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\n\ntreated = df[df['treatment'] == 1]\ncontrol = df[df['treatment'] == 0]\n\ntable1_vars = [\n    'mrm2',    \n    'hpa',      \n    'freq',      \n    'years',     \n    'year5',     \n    'female', 'couple',\n    'redcty',\n    'nonlit', 'cases'\n]\n\ndef manual_ttest(var):\n    Xt, Xc = treated[var], control[var]\n    mean_diff = Xt.mean() - Xc.mean()\n    pooled_se = np.sqrt(Xt.var(ddof=1)/len(Xt) + Xc.var(ddof=1)/len(Xc))\n    t_stat = mean_diff / pooled_se\n    return {\n        \"Variable\": var,\n        \"Mean_Treatment\": round(Xt.mean(), 3),\n        \"Mean_Control\": round(Xc.mean(), 3),\n        \"t-stat\": round(t_stat, 4)\n    }\n\nmanual_ttest_df = pd.DataFrame([manual_ttest(v) for v in table1_vars])\nmanual_ttest_df\n\n\n\n\n\n\n\n\n\nVariable\nMean_Treatment\nMean_Control\nt-stat\n\n\n\n\n0\nmrm2\n13.012\n12.998000\n0.1195\n\n\n1\nhpa\n59.597\n58.959999\n0.9704\n\n\n2\nfreq\n8.035\n8.047000\n-0.1108\n\n\n3\nyears\n6.078\n6.136000\n-1.0909\n\n\n4\nyear5\n0.506\n0.514000\n-1.5627\n\n\n5\nfemale\n0.275\n0.283000\n-1.7727\n\n\n6\ncouple\n0.091\n0.093000\n-0.5888\n\n\n7\nredcty\n0.512\n0.507000\n0.9050\n\n\n8\nnonlit\n2.485\n2.453000\n1.7132\n\n\n9\ncases\n1.499\n1.502000\n-0.3428\n\n\n\n\n\n\n\nThis table replicates the balance check presented in Table 1 of Karlan & List (2007), using a manual t-test formula to compare pre-treatment characteristics across the treatment and control groups. The variables cover donor behavior (e.g., months since last donation, prior contributions), demographics (e.g., gender, race, household size), and political/geographic indicators (e.g., red state/county, legal involvement of the organization). Across all tested variables, the t-statistics remain small (generally below ±2), and none of the differences reach conventional levels of statistical significance. This provides strong evidence that the treatment assignment was effectively randomized and groups are balanced on observable characteristics. This confirms the reliability of the experiment design and supports the validity of later comparisons on donation behavior.\n\n\nShow code\n# from tabulate import tabulate\nimport statsmodels.formula.api as smf\n\nreg_results = []\nfor var in table1_vars:\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    reg_results.append([\n        var,\n        round(model.params[\"treatment\"], 4),\n        round(model.pvalues[\"treatment\"], 4),\n        round(model.rsquared, 4)\n    ])\n\n# headers = [\"Variable\", \"Treatment Coef\", \"p-value\", \"R-squared\"]\n# print(tabulate(reg_results, headers=headers, tablefmt=\"github\"))\nreg_df = pd.DataFrame(reg_results)\nreg_df  \n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\nmrm2\n0.0137\n0.9049\n0.0000\n\n\n1\nhpa\n0.6371\n0.3451\n0.0000\n\n\n2\nfreq\n-0.0120\n0.9117\n0.0000\n\n\n3\nyears\n-0.0575\n0.2700\n0.0000\n\n\n4\nyear5\n-0.0074\n0.1182\n0.0000\n\n\n5\nfemale\n-0.0075\n0.0787\n0.0001\n\n\n6\ncouple\n-0.0016\n0.5594\n0.0000\n\n\n7\nredcty\n0.0043\n0.3659\n0.0000\n\n\n8\nnonlit\n0.0318\n0.0888\n0.0001\n\n\n9\ncases\n-0.0037\n0.7333\n0.0000\n\n\n\n\n\n\n\nThis regression-based balance check complements the earlier t-tests by estimating the relationship between treatment assignment and baseline covariates. For each variable, we regress it on the treatment indicator and inspect the coefficient and p-value. All p-values are well above the 0.05 threshold, confirming that treatment status is not significantly associated with any pre-treatment variable. This is consistent with the design of a randomized controlled trial, further supporting the internal validity of the experiment. The results also match those from the manual t-tests, which is expected since the regression with a binary independent variable produces the same mean difference and inference as a t-test."
  },
  {
    "objectID": "blogs/hw1/index.html#experimental-results",
    "href": "blogs/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nShow code\nimport matplotlib.pyplot as plt\n\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\ndonation_rates.plot(kind=\"bar\", color=[\"green\", \"orange\"])\nplt.title(\"Proportion of People Who Donated\")\nplt.ylabel(\"Donation Rate\")\nplt.xticks(rotation=0)\nplt.ylim(0, 0.035)  \nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot displays the proportion of individuals who made a charitable contribution in the control and treatment groups. The donation rate is noticeably higher in the treatment group, indicating that the presence of a matching donation offer likely increased the likelihood of giving. This supports the hypothesis that matched donations can serve as an effective behavioral nudge in charitable fundraising.\n\n\nShow code\nfrom scipy.stats import ttest_ind\n\ntreated = df[df['treatment'] == 1]\ncontrol = df[df['treatment'] == 0]\n\nt_stat, p_val = ttest_ind(treated[\"gave\"], control[\"gave\"], equal_var=False)\n\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n\nt-statistic: 3.2095, p-value: 0.0013\n\n\n\n\nShow code\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\n\nfrom tabulate import tabulate\ntable = [[\"Coef\", model.params[\"treatment\"]],\n         [\"p-value\", model.pvalues[\"treatment\"]],\n         [\"R-squared\", model.rsquared]]\n# print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\ndf_table = pd.DataFrame(table)\ndf_table \n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nCoef\n0.004180\n\n\n1\np-value\n0.001927\n\n\n2\nR-squared\n0.000192\n\n\n\n\n\n\n\nBoth the t-test and linear regression show a statistically significant difference in donation behavior between the treatment and control groups. Individuals who received a letter mentioning a matching grant were more likely to donate than those who received a standard letter. This finding replicates Table 2a Panel A of Karlan & List (2007), and supports the hypothesis that matched donations act as a strong psychological motivator. From a behavioral standpoint, it suggests that people are more inclined to give when their contributions feel amplified — they may view their gift as having a greater impact, which encourages them to take action. This simple intervention—adding a match offer—significantly influenced behavior even though the individuals in both groups received similar messages aside from that detail.\n\n\nShow code\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom tabulate import tabulate\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=0)\n\nprobit_summary = [\n    [\"Coef\", probit_model.params[\"treatment\"]],\n    [\"p-value\", probit_model.pvalues[\"treatment\"]],\n    [\"Pseudo R-squared\", probit_model.prsquared]\n]\n#print(tabulate(probit_summary, headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\ndf_probit_summary = pd.DataFrame(probit_summary)\ndf_probit_summary\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nCoef\n0.086785\n\n\n1\np-value\n0.001852\n\n\n2\nPseudo R-squared\n0.000978\n\n\n\n\n\n\n\nThis probit regression estimates the effect of treatment assignment on the likelihood of making a donation, using a nonlinear model suitable for binary outcomes. The coefficient on the treatment variable is positive and statistically significant, indicating that individuals who received a matching gift message were more likely to donate. This result replicates the findings of Table 3, Column 1 in Karlan & List (2007), providing further evidence that matching donations increase participation. While the probit coefficient itself isn’t directly interpretable as a percentage change, its sign and significance confirm the effect observed in the linear model and t-test. Together, these consistent results across models support the conclusion that match framing can significantly increase donor engagement in charitable campaigns.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nShow code\nfrom scipy.stats import ttest_ind\n\nmatch_only = df[df[\"treatment\"] == 1]\n\nratio1 = match_only[match_only[\"ratio\"] == \"ratio1\"]\nratio2 = match_only[match_only[\"ratio\"] == \"ratio2\"]\nratio3 = match_only[match_only[\"ratio\"] == \"ratio3\"]\n\ntests = {\n    \"2:1 vs 1:1\": ttest_ind(ratio2[\"gave\"], ratio1[\"gave\"], equal_var=False),\n    \"3:1 vs 1:1\": ttest_ind(ratio3[\"gave\"], ratio1[\"gave\"], equal_var=False),\n    \"3:1 vs 2:1\": ttest_ind(ratio3[\"gave\"], ratio2[\"gave\"], equal_var=False),\n}\n\nimport pandas as pd\nttest_results = pd.DataFrame([\n    [k, round(v[0], 4), round(v[1], 4)]\n    for k, v in tests.items()\n], columns=[\"Comparison\", \"t-statistic\", \"p-value\"])\n\nttest_results\n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\n\n\n\n\n0\n2:1 vs 1:1\nNaN\nNaN\n\n\n1\n3:1 vs 1:1\nNaN\nNaN\n\n\n2\n3:1 vs 2:1\nNaN\nNaN\n\n\n\n\n\n\n\nThe t-tests above examine whether increasing the match ratio (from 1:1 to 2:1 or 3:1) significantly affects the probability of donating. The results show that the differences in donation rates are not statistically significant** at conventional levels. This finding aligns with the authors’ comment on page 8 of Karlan & List (2007), where they state that “the figures suggest that higher match ratios did not lead to significantly greater giving rates.” Despite intuitive expectations, offering a 2:1 or 3:1 match did not lead to more people donating compared to a 1:1 match. This suggests that the psychological impact of having any match available may be more important than the generosity of the multiplier itself.\n\n\nShow code\nimport statsmodels.formula.api as smf\nfrom tabulate import tabulate\n\nmatched = df[df[\"treatment\"] == 1]\n\nmodel = smf.ols(\"gave ~ C(ratio)\", data=matched).fit()\n\nsummary_table = [\n    [\"Intercept (ratio1)\", model.params[\"Intercept\"], model.pvalues[\"Intercept\"]],\n    [\"ratio2\", model.params.get(\"C(ratio)[T.ratio2]\", 0), model.pvalues.get(\"C(ratio)[T.ratio2]\", 1)],\n    [\"ratio3\", model.params.get(\"C(ratio)[T.ratio3]\", 0), model.pvalues.get(\"C(ratio)[T.ratio3]\", 1)],\n]\n#print(tabulate(summary_table, headers=[\"Term\", \"Coef\", \"p-value\"], tablefmt=\"github\"))\ndf_summary_table = pd.DataFrame(summary_table)\ndf_summary_table\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\nIntercept (ratio1)\n1.230440e+09\n0.912402\n\n\n1\nratio2\n0.000000e+00\n1.000000\n\n\n2\nratio3\n0.000000e+00\n1.000000\n\n\n\n\n\n\n\nThis regression estimates the donation probability under different match ratios, using ratio1 (1:1 match) as the baseline category. The intercept represents the average donation rate for the 1:1 group, while the coefficients for ratio2 and ratio3 represent the change in probability relative to that baseline. The results confirm that there are no statistically significant differences in donation rates when comparing 2:1 or 3:1 match ratios to the 1:1 ratio. These coefficients are close to zero and accompanied by large p-values, reinforcing the earlier t-test findings. In line with the authors’ interpretation, this suggests that increasing the match ratio beyond 1:1 does not further motivate giving** — the existence of a match alone may be the primary driver of behavioral change, rather than its size.\n\n\nShow code\ncoef_ratio1 = model.params[\"Intercept\"]\ncoef_ratio2 = model.params.get(\"C(ratio)[T.ratio2]\", 0)\ncoef_ratio3 = model.params.get(\"C(ratio)[T.ratio3]\", 0)\n\ndiff_2vs1 = coef_ratio2\ndiff_3vs2 = coef_ratio3 - coef_ratio2\n\nimport pandas as pd\nresponse_diff = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\"],\n    \"Difference in Fitted Rate\": [round(diff_2vs1, 5), round(diff_3vs2, 5)]\n})\n\nresponse_diff\n\n\n\n\n\n\n\n\n\nComparison\nDifference in Fitted Rate\n\n\n\n\n0\n2:1 vs 1:1\n0\n\n\n1\n3:1 vs 2:1\n0\n\n\n\n\n\n\n\nThis table shows the difference in predicted donation rates based on the fitted values from the regression model. - The difference between the 2:1 and 1:1 match groups is very small and not statistically significant. - The difference between the 3:1 and 2:1 groups is similarly negligible.\nThese findings provide further evidence that increasing the match ratio beyond 1:1 does not meaningfully increase the likelihood of donating. The presence of a match appears to matter, but its size does not — supporting the idea that the psychological nudge of a match is more about its existence than its magnitude. This aligns with the interpretation found in Karlan & List (2007), where the authors argue that the offer of any match (even 1:1) seems sufficient to trigger the intended behavioral response.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\nShow code\nfrom scipy.stats import ttest_ind\n\nt_stat, p_val = ttest_ind(treated[\"amount\"], control[\"amount\"], equal_var=False)\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n\nt-statistic: 1.9183, p-value: 0.0551\n\n\n\n\nShow code\nimport statsmodels.formula.api as smf\nfrom tabulate import tabulate\n\nmodel_amt = smf.ols(\"amount ~ treatment\", data=df).fit()\n\nsummary_table = [\n    [\"Intercept (Control)\", model_amt.params[\"Intercept\"], model_amt.pvalues[\"Intercept\"]],\n    [\"Treatment Coef\", model_amt.params[\"treatment\"], model_amt.pvalues[\"treatment\"]],\n    [\"R-squared\", model_amt.rsquared, \"\"]\n]\n# print(tabulate(summary_table, headers=[\"Term\", \"Coef\", \"p-value\"], tablefmt=\"github\"))\ndf_summary_table = pd.DataFrame(summary_table)\ndf_summary_table\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\nIntercept (Control)\n0.813268\n0.0\n\n\n1\nTreatment Coef\n0.153605\n0.06282\n\n\n2\nR-squared\n0.000069\n\n\n\n\n\n\n\n\nThis analysis tests whether the average donation amount differs between the treatment and control groups — regardless of whether someone donated or not (i.e., this includes all the zeros). Both the t-test and linear regression show that the mean donation amount is slightly higher in the treatment group, but the difference is small in magnitude and statistically weak. The low R-squared value also indicates that treatment status explains very little of the variance in donation amounts. This suggests that while matched donations may influence whether people give, they don’t strongly influence how much they give, at least when considering all individuals (including those who gave $0). To better understand donation behavior among actual donors, we’ll refine this analysis next by limiting it to individuals who did donate.\n\n\nShow code\ndonors_only = df[df[\"gave\"] == 1]\n\nmodel_conditional = smf.ols(\"amount ~ treatment\", data=donors_only).fit()\n\nsummary_table = [\n    [\"Intercept (Control)\", model_conditional.params[\"Intercept\"], model_conditional.pvalues[\"Intercept\"]],\n    [\"Treatment Coef\", model_conditional.params[\"treatment\"], model_conditional.pvalues[\"treatment\"]],\n    [\"R-squared\", model_conditional.rsquared, \"\"]\n]\n#print(tabulate(summary_table, headers=[\"Term\", \"Coef\", \"p-value\"], tablefmt=\"github\"))\ndf_summary_table = pd.DataFrame(summary_table)\ndf_summary_table\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\nIntercept (Control)\n45.540268\n0.0\n\n\n1\nTreatment Coef\n-1.668393\n0.561476\n\n\n2\nR-squared\n0.000327\n\n\n\n\n\n\n\n\nBy focusing only on those who actually donated, this regression estimates how much more (or less) people gave if they were in the treatment group compared to the control group. The estimated coefficient on treatment now reflects the difference in average gift size conditional on donating. The result shows that the treatment has very little effect on the amount donated among those who gave something — the coefficient is small and statistically insignificant. This finding supports the idea that the matching grant mainly affects the decision to donate, not the amount donated once that decision is made. In other words, treatment increases the extensive margin (whether to give), but not the intensive margin (how much to give). Regarding causal interpretation: since treatment was randomly assigned, and we’re conditioning on an outcome (gave), the estimate does not have a strict causal interpretation. It’s subject to selection bias — those who gave in treatment might be different in unobservable ways from those who gave in control. So we interpret this descriptively rather than causally.\n\n\nShow code\nimport matplotlib.pyplot as plt\n\ndonors_only = df[df[\"gave\"] == 1]\n\ntreat_donors = donors_only[donors_only[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = donors_only[donors_only[\"treatment\"] == 0][\"amount\"]\n\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Treatment plot\naxes[0].hist(treat_donors, bins=30, color='orange', edgecolor='black', alpha=0.8)\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f'Mean = {mean_treat:.2f}')\naxes[0].set_title(\"Treatment Group\")\naxes[0].set_xlabel(\"Donation Amount ($)\")\naxes[0].set_ylabel(\"Number of Donors\")\naxes[0].legend()\n\n# Control plot\naxes[1].hist(control_donors, bins=30, color='green', edgecolor='black', alpha=0.8)\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f'Mean = {mean_control:.2f}')\naxes[1].set_title(\"Control Group\")\naxes[1].set_xlabel(\"Donation Amount ($)\")\naxes[1].legend()\n\nfig.suptitle(\"Histogram of Donation Amounts (Among Donors Only)\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese histograms visualize the distribution of donation amounts among those who gave, separately for the treatment and control groups. The red vertical lines represent the sample average donation in each group. Below are the main observations: - Both distributions are highly right-skewed with many small donations and a few large outliers. - The average donation amount is slightly lower in the treatment group than in the control group. These observations align with earlier regression findings showing that while matched gifts increase the likelihood of donating, they do not increase the amount given by those who choose to donate. This suggests that matching gifts influence whether people give, but not how much they give, reinforcing the psychological interpretation that matched gifts are more about prompting action than amplifying generosity."
  },
  {
    "objectID": "blogs/hw1/index.html#simulation-experiment",
    "href": "blogs/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ttest_ind\n\np_control = 0.018\np_treat = 0.022\nn = 1000             \niterations = 10000   \n\nt_stats = []\n\nfor _ in range(iterations):\n    control_sample = np.random.binomial(1, p_control, n)\n    treat_sample = np.random.binomial(1, p_treat, n)\n    t_stat, _ = ttest_ind(treat_sample, control_sample, equal_var=False)\n    t_stats.append(t_stat)\n\nplt.figure(figsize=(10, 5))\nplt.hist(t_stats, bins=50, color='orange', edgecolor='black', density=True)\nplt.axvline(np.mean(t_stats), color='red', linestyle='--', label=f\"Mean t = {np.mean(t_stats):.3f}\")\nplt.axvline(1.96, color='gray', linestyle=':', label='95% Critical Value')\nplt.axvline(-1.96, color='gray', linestyle=':')\nplt.title(\"Simulated Distribution of t-Statistics\")\nplt.xlabel(\"t-statistic\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis simulation mimics running 10,000 randomized experiments comparing donation rates between a control group (with p = 0.018) and a treatment group (with p = 0.022), each with 1,000 individuals. I calculate the t-statistic for the difference in proportions in each simulated trial. The resulting histogram approximates the sampling distribution of the t-statistic under these parameters. The following are the key observations based on the simulation result: - The distribution is centered slightly above 0, reflecting the small true difference in means (0.004). - It approximates a normal distribution, which is expected from the Central Limit Theorem. - The proportion of simulated t-statistics exceeding ±1.96 shows how often a standard hypothesis test would reject the null of no difference at the 5% level.\nThis visually demonstrates how small but real differences can become detectable with sufficient sample size and repeated sampling — a core insight from the Law of Large Numbers and Central Limit Theorem.\n\nLaw of Large Numbers\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 10000\np_control = 0.018\np_treat = 0.022\n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treat, n)\n\ndiff = treatment - control\ncumulative_avg = np.cumsum(diff) / np.arange(1, n + 1)\ntrue_diff = p_treat - p_control\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, color='orange', lw=2, label=\"Cumulative Average\")\nplt.axhline(true_diff, color='red', linestyle='--', label=f\"True Diff = {true_diff:.4f}\")\nplt.title(\"Law of Large Numbers: Convergence of Mean Differences\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Difference (Treatment - Control)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot illustrates how the cumulative average difference in donation outcomes between treatment and control groups converges to the true difference in means (0.004) as more data is accumulated. Each point on the blue line shows the average difference in outcomes after that many paired observations. As the sample size increases, the average stabilizes — a clear demonstration of the Law of Large Numbers in action. The dashed red line marks the true difference in the population, and we see that the sample-based average gets increasingly close to it. This reflects why larger experiments give more reliable, less variable estimates.\n\n\nCentral Limit Theorem\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np_control = 0.018\np_treat = 0.022\ntrue_diff = p_treat - p_control\nsample_sizes = [50, 200, 500, 1000]\niterations = 1000\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    for _ in range(iterations):\n        ctrl = np.random.binomial(1, p_control, n)\n        treat = np.random.binomial(1, p_treat, n)\n        mean_diffs.append(np.mean(treat) - np.mean(ctrl))\n    \n    axs[i].hist(mean_diffs, bins=30, color='orange', edgecolor='black')\n    axs[i].axvline(true_diff, color='red', linestyle='--', label=f\"True diff = {true_diff:.4f}\")\n    axs[i].axvline(0, color='black', linestyle=':', label=\"Zero\")\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].legend()\n    axs[i].set_xlabel(\"Sample Mean Differences\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].grid(True, linestyle='--', alpha=0.5)\n\nfig.suptitle(\"Central Limit Theorem: Distribution of Sample Mean Differences\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nEach histogram shows the distribution of sample mean differences between treatment and control groups from 1000 repeated experiments. The sample size increases from 50 to 1000 across the four plots. The following are the key takeaways: - With small samples (like ( n = 50 )), the distribution of average differences is wide and rough, with a high chance of extreme values. - As sample size increases, the distribution becomes smoother, more symmetric, and tightly clustered around the true difference(0.004). - This is a clear visual demonstration of the Central Limit Theorem: even though each individual donation is binary (0 or 1), the distribution of the sample mean becomes approximately normal for large ( n ). - Importantly, zero is not centered in the distribution, indicating that there is a real effect and that repeated experiments would often detect it."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, xc xmtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Analysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nMay 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blogs/hw3/index.html",
    "href": "blogs/hw3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blogs/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blogs/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blogs/hw3/index.html#simulate-conjoint-data",
    "href": "blogs/hw3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nimport itertools\n\nnp.random.seed(123)\n\n# Define attributes\nbrand = ['N', 'P', 'H']  # Netflix, Prime, Hulu\nad = ['Yes', 'No']\nprice = np.arange(8, 36, 4)\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(list(itertools.product(brand, ad, price)), columns=['brand', 'ad', 'price'])\nm = len(profiles)\n\n# Assign part-worth utilities\nb_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}\na_util = {'Yes': -0.8, 'No': 0.0}\np_util = lambda p: -0.1 * p\n\n# Parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent's data\ndef sim_one(id):\n    dat_list = []\n\n    for t in range(1, n_tasks + 1):\n        sample_profiles = profiles.sample(n=n_alts).copy()\n        sample_profiles['resp'] = id\n        sample_profiles['task'] = t\n\n        # Deterministic utility\n        v = (\n            sample_profiles['brand'].map(b_util)\n            + sample_profiles['ad'].map(a_util)\n            + p_util(sample_profiles['price'])\n        )\n\n        # Add Gumbel noise (Type I extreme value)\n        e = -np.log(-np.log(np.random.rand(n_alts)))\n        u = v + e\n\n        # Identify chosen alternative\n        choice = (u == u.max()).astype(int)\n        sample_profiles['choice'] = choice\n\n        dat_list.append(sample_profiles)\n\n    return pd.concat(dat_list)\n\n# Simulate all respondents\ndf_list = [sim_one(i + 1) for i in range(n_peeps)]\nconjoint_data = pd.concat(df_list).reset_index(drop=True)\n\n# Keep only observable variables\nconjoint_data = conjoint_data[['resp', 'task', 'brand', 'ad', 'price', 'choice']]"
  },
  {
    "objectID": "blogs/hw3/index.html#preparing-the-data-for-estimation",
    "href": "blogs/hw3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data\n\n\nShow code\n# One-hot encode brand and ad\nX = pd.get_dummies(conjoint_data, columns=['brand', 'ad'], drop_first=True)\n\n# Optional: rename columns for clarity\nX.rename(columns={\n    'brand_P': 'brand_Prime',\n    'brand_H': 'brand_Hulu',\n    'ad_Yes': 'ad_Yes'\n}, inplace=True)\n\n# View the prepared data\nX.head()\n\n\n\n\n\n\n\n\n\nresp\ntask\nprice\nchoice\nbrand_N\nbrand_Prime\nad_Yes\n\n\n\n\n0\n1\n1\n32\n0\nFalse\nTrue\nFalse\n\n\n1\n1\n1\n28\n0\nTrue\nFalse\nFalse\n\n\n2\n1\n1\n24\n1\nTrue\nFalse\nFalse\n\n\n3\n1\n2\n28\n0\nFalse\nFalse\nFalse\n\n\n4\n1\n2\n8\n1\nFalse\nFalse\nFalse"
  },
  {
    "objectID": "blogs/hw3/index.html#estimation-via-maximum-likelihood",
    "href": "blogs/hw3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval."
  },
  {
    "objectID": "blogs/hw3/index.html#estimation-via-bayesian-methods",
    "href": "blogs/hw3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach."
  },
  {
    "objectID": "blogs/hw3/index.html#discussion",
    "href": "blogs/hw3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  }
]