[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "My Blogs",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nLebin Sun\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\nLebin Sun\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nLebin Sun\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nLebin Sun\n\n\nInvalid Date\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lebin Sun",
    "section": "",
    "text": "Welcome to my website!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blogs/hw3/index.html",
    "href": "blogs/hw3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blogs/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blogs/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blogs/hw3/index.html#simulate-conjoint-data",
    "href": "blogs/hw3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nimport itertools\n\nnp.random.seed(123)\n\n# Define attributes\nbrand = ['N', 'P', 'H']  # Netflix, Prime, Hulu\nad = ['Yes', 'No']\nprice = np.arange(8, 36, 4)\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(list(itertools.product(brand, ad, price)), columns=['brand', 'ad', 'price'])\nm = len(profiles)\n\n# Assign part-worth utilities\nb_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}\na_util = {'Yes': -0.8, 'No': 0.0}\np_util = lambda p: -0.1 * p\n\n# Parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent's data\ndef sim_one(id):\n    dat_list = []\n\n    for t in range(1, n_tasks + 1):\n        sample_profiles = profiles.sample(n=n_alts).copy()\n        sample_profiles['resp'] = id\n        sample_profiles['task'] = t\n\n        # Deterministic utility\n        v = (\n            sample_profiles['brand'].map(b_util)\n            + sample_profiles['ad'].map(a_util)\n            + p_util(sample_profiles['price'])\n        )\n\n        # Add Gumbel noise (Type I extreme value)\n        e = -np.log(-np.log(np.random.rand(n_alts)))\n        u = v + e\n\n        # Identify chosen alternative\n        choice = (u == u.max()).astype(int)\n        sample_profiles['choice'] = choice\n\n        dat_list.append(sample_profiles)\n\n    return pd.concat(dat_list)\n\n# Simulate all respondents\ndf_list = [sim_one(i + 1) for i in range(n_peeps)]\nconjoint_data = pd.concat(df_list).reset_index(drop=True)\n\n# Keep only observable variables\nconjoint_data = conjoint_data[['resp', 'task', 'brand', 'ad', 'price', 'choice']]"
  },
  {
    "objectID": "blogs/hw3/index.html#preparing-the-data-for-estimation",
    "href": "blogs/hw3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nShow code\nimport pandas as pd\n\n# Load data if needed (skip this if already in memory)\ndf = pd.read_csv(\"conjoint_data.csv\")\n\n# Drop one dummy per variable to avoid multicollinearity\ndf_encoded = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n\n# Prepare final design matrix\nfeature_cols = ['price', 'brand_P', 'brand_N', 'ad_Yes']\nX_data = df_encoded.copy()\nX_data[feature_cols] = X_data[feature_cols].astype(float)\nX = X_data[feature_cols].values\n\n# Target variable: 1 if alternative was chosen, 0 otherwise\ny = X_data['choice'].values\n\n# Create group ID per respondent-task (i.e., choice set)\ngroup_labels = X_data['resp'].astype(str) + \"_\" + X_data['task'].astype(str)\ngroup_ids = pd.factorize(group_labels)[0]"
  },
  {
    "objectID": "blogs/hw3/index.html#estimation-via-maximum-likelihood",
    "href": "blogs/hw3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo estimate the parameters of the Multinomial Logit (MNL) model via maximum likelihood, we first need to specify the log-likelihood function. The MNL model assumes that the probability of a respondent choosing a particular alternative is a function of the observed attributes of that alternative and the model coefficients. The log-likelihood sums the log of these probabilities across all observed choices.\nBelow, we define a Python function that computes the negative log-likelihood, which will later be minimized using numerical optimization techniques.\n\n\nShow code\nfrom scipy.special import logsumexp\nimport numpy as np\n\ndef neg_log_likelihood(beta, X, y, group_ids):\n    \"\"\"\n    Computes the negative log-likelihood for a Multinomial Logit model.\n\n    Parameters:\n        beta: array-like, coefficients for each covariate\n        X: design matrix of covariates\n        y: binary array where 1 indicates the chosen alternative\n        group_ids: array marking which rows belong to the same choice set\n\n    Returns:\n        Negative log-likelihood (float)\n    \"\"\"\n    utility = X @ beta\n    log_prob = np.empty_like(utility)\n\n    for g in np.unique(group_ids):\n        idx = group_ids == g\n        denom = logsumexp(utility[idx])\n        log_prob[idx] = utility[idx] - denom\n\n    return -np.sum(log_prob[y == 1])\n\n\nThis function computes the probability of the chosen alternative within each choice task and returns the (negative) sum of their log-probabilities.\nWith the log-likelihood function defined, we now use the scipy.optimize.minimize() function to find the Maximum Likelihood Estimates (MLEs) of the model parameters. The optimization routine minimizes the negative log-likelihood, and the inverse of the Hessian matrix gives us the variance-covariance matrix for the parameter estimates. From this, we compute standard errors and construct 95% confidence intervals for each coefficient.\n\n\nShow code\nfrom scipy.optimize import minimize\nimport pandas as pd\n\n# Initial guess for beta\ninit_beta = np.zeros(X.shape[1])\n\n# Perform MLE via BFGS optimization\nresult = minimize(neg_log_likelihood, init_beta, args=(X, y, group_ids), method='BFGS')\n\n# Extract coefficient estimates\nbeta_hat = result.x\n\n# Compute standard errors from the inverse Hessian\nhessian_inv = result.hess_inv\nse = np.sqrt(np.diag(hessian_inv))\n\n# 95% confidence intervals\nci_lower = beta_hat - 1.96 * se\nci_upper = beta_hat + 1.96 * se\n\n# Create results DataFrame\nfeature_cols = ['price', 'brand_P', 'brand_N', 'ad_Yes']\nresults_df = pd.DataFrame({\n    'Parameter': feature_cols,\n    'Estimate': beta_hat,\n    'StdError': se,\n    '95% CI Lower': ci_lower,\n    '95% CI Upper': ci_upper\n})\n\nresults_df\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStdError\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nprice\n-0.099480\n0.006332\n-0.111892\n-0.087069\n\n\n1\nbrand_P\n0.501616\n0.120785\n0.264878\n0.738354\n\n\n2\nbrand_N\n0.941195\n0.114119\n0.717522\n1.164868\n\n\n3\nad_Yes\n-0.731994\n0.089070\n-0.906572\n-0.557416\n\n\n\n\n\n\n\nThis output shows the estimated effect of each product attribute on utility. Positive coefficients indicate higher utility (and choice probability), while negative values indicate a deterrent effect. For example, a negative price coefficient implies that higher prices reduce choice probability."
  },
  {
    "objectID": "blogs/hw3/index.html#estimation-via-bayesian-methods",
    "href": "blogs/hw3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nTo estimate the posterior distribution of the MNL parameters, we implement a Metropolis-Hastings (M-H) algorithm. We use a random walk proposal distribution based on a multivariate normal with small variances. The posterior is proportional to the product of the likelihood and the prior; in log-space, this becomes a sum.\nFor priors, we assume: - ({price} (0, 1)) - ({binary} (0, 5)) for all binary variables\n\n\nShow code\n# Log-posterior function = log-likelihood + log-prior\ndef log_posterior(beta, X, y, group_ids):\n    ll = -neg_log_likelihood(beta, X, y, group_ids)  # from earlier section\n\n    # Prior: N(0,1) for price; N(0,5) for binary variables\n    log_prior = -0.5 * (beta[0] ** 2) / (1**2)  # price\n    log_prior += -0.5 * np.sum((beta[1:] ** 2) / (5**2))  # other variables\n\n    return ll + log_prior\n\n\nNext, we implement the Metropolis-Hastings sampler. At each step, a new proposal is drawn from a multivariate normal distribution centered at the current value. If the new value increases the posterior, it is always accepted; otherwise, it is accepted with a probability proportional to the ratio of the new and current posterior values.\nWe run the chain for 11,000 steps and discard the first 1,000 as burn-in, leaving 10,000 posterior samples.\n\n\nShow code\ndef metropolis_sampler(log_post_fn, X, y, group_ids, init, steps=11000, burn=1000, proposal_sd=0.05):\n    n_params = len(init)\n    draws = np.zeros((steps, n_params))\n    current = init.copy()\n    current_lp = log_post_fn(current, X, y, group_ids)\n\n    for s in range(steps):\n        proposal = current + np.random.normal(scale=proposal_sd, size=n_params)\n        proposal_lp = log_post_fn(proposal, X, y, group_ids)\n        acc_ratio = np.exp(proposal_lp - current_lp)\n\n        if np.random.rand() &lt; acc_ratio:\n            current = proposal\n            current_lp = proposal_lp\n        draws[s] = current\n\n    return draws[burn:]  # discard burn-in\n\n\nWe now run the sampler, using a zero vector as the initial value and a small step size to control the proposal distribution. The resulting draws form an approximate sample from the true posterior distribution.\n\n\nShow code\ninit_beta = np.zeros(X.shape[1])\nsamples = metropolis_sampler(log_posterior, X, y, group_ids, init_beta)\n\nimport pandas as pd\nposterior_df = pd.DataFrame(samples, columns=['price', 'brand_P', 'brand_N', 'ad_Yes'])\n\n\nTo assess the behavior of the Metropolis-Hastings sampler, we visualize its output for the price coefficient. The trace plot shows the sequence of sampled values over iterations and helps diagnose convergence and mixing. The histogram displays the marginal posterior distribution and allows us to visually interpret central tendency and spread.\n\n\nShow code\nimport matplotlib.pyplot as plt\n\ndef plot_trace_hist(samples, param_name):\n    fig, ax = plt.subplots(2, 1, figsize=(8, 6), constrained_layout=True)\n\n    # Trace plot\n    ax[0].plot(samples, color='steelblue', linewidth=0.8)\n    ax[0].set_title(f'Trace Plot for β_{param_name}')\n    ax[0].set_ylabel('Value')\n    ax[0].set_xlabel('Iteration')\n\n    # Histogram\n    ax[1].hist(samples, bins=30, color='salmon', edgecolor='black', alpha=0.8)\n    ax[1].set_title(f'Posterior Distribution for β_{param_name}')\n    ax[1].set_xlabel('Value')\n    ax[1].set_ylabel('Frequency')\n\n    plt.show()\n\n# Plot for price\nplot_trace_hist(posterior_df['price'], 'price')\n\n\n\n\n\n\n\n\n\nFrom the price trace plot, we observe that the sampler stabilizes after the burn-in period and mixes well throughout the chain. The histogram shows that the posterior distribution for ( _{} ) is centered around a negative value, consistent with our economic expectation that higher prices reduce choice probability.\n\n\nShow code\nplot_trace_hist(posterior_df['brand_P'], 'brand_P')\n\n\n\n\n\n\n\n\n\nThe trace plot for ( _{} ) shows good mixing behavior and no apparent trends or drifts. The posterior distribution indicates a positive effect of Prime on the likelihood of choice, suggesting that Prime is preferred relative to the baseline brand. However, the distribution has more spread compared to price, indicating greater uncertainty.\n\n\nShow code\nplot_trace_hist(posterior_df['brand_P'], 'brand_N')\n\n\n\n\n\n\n\n\n\nFor ( _{} ), the trace plot is well-centered and shows tight convergence. The histogram shows a strongly positive distribution with little spread, suggesting that Netflix is consistently preferred across respondents. The density’s concentration implies high certainty about this positive utility effect.\n\n\nShow code\nplot_trace_hist(posterior_df['ad_Yes'], 'ad_Yes')\n\n\n\n\n\n\n\n\n\nThe trace for ( _{} ) is stable and shows good mixing. The histogram is clearly left-skewed and centered below zero, indicating that the presence of an ad significantly reduces utility and likelihood of selection. This effect is both substantial and precisely estimated, as reflected in the sharp posterior peak.\nWe now summarize the Bayesian posterior draws by calculating the mean, standard deviation, and 95% credible interval for each parameter. These summaries provide a direct comparison to the MLE results from Section 4.\nThe 95% credible interval is computed as the 2.5th and 97.5th percentiles of the posterior samples. This reflects the Bayesian interpretation of uncertainty: there is a 95% probability that the true parameter lies within this interval, given the data and prior.\n\n\nShow code\n# Posterior summaries\nposterior_summary = pd.DataFrame({\n    'Parameter': posterior_df.columns,\n    'Posterior Mean': posterior_df.mean().values,\n    'Posterior SD': posterior_df.std().values,\n    '2.5%': posterior_df.quantile(0.025).values,\n    '97.5%': posterior_df.quantile(0.975).values\n})\n\nposterior_summary\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior SD\n2.5%\n97.5%\n\n\n\n\n0\nprice\n-0.100241\n0.006413\n-0.112832\n-0.087806\n\n\n1\nbrand_P\n0.489580\n0.113299\n0.269923\n0.719313\n\n\n2\nbrand_N\n0.947974\n0.108354\n0.749385\n1.176308\n\n\n3\nad_Yes\n-0.757367\n0.094581\n-0.944148\n-0.565816\n\n\n\n\n\n\n\nComparing these results to the MLE estimates in Section 4, we generally expect: - Similar posterior means and MLE point estimates, particularly when the priors are weakly informative and the sample size is large. - Slightly wider credible intervals compared to MLE confidence intervals, as the Bayesian intervals incorporate prior uncertainty.\nThese comparisons allow us to verify the robustness of inference under both estimation approaches."
  },
  {
    "objectID": "blogs/hw3/index.html#discussion",
    "href": "blogs/hw3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we did not know the data were simulated, we could still draw several substantive conclusions from the parameter estimates:\n\nThe fact that ( {} &gt; {} ) suggests that, on average, respondents preferred Netflix over Prime. This interpretation is supported by the magnitude and significance of the estimated coefficients, both in the MLE and Bayesian frameworks. The larger value for Netflix indicates higher utility, all else equal.\nThe negative sign of ( _{} ) is both intuitive and expected: as the price of a streaming service increases, the likelihood of that service being chosen decreases. This reflects standard economic behavior, where higher costs tend to reduce demand.\nThe direction and relative sizes of the coefficients align well with plausible consumer preferences. Even without knowing the ground truth (as in simulation), these estimates would provide credible guidance on which features drive choice.\n\nOverall, the parameter estimates are interpretable, consistent with economic theory, and aligned with real-world consumer behavior. This supports the face validity of the model, even without knowing the simulation structure behind the data.\nWhile the parameter estimates we obtained are informative, they reflect average preferences across all respondents. In practice, however, individuals often differ in how they value features like price or brand. This motivates the use of multi-level (hierarchical) models, which account for preference heterogeneity across respondents.\nTo simulate and estimate a multi-level (also called random-parameter or hierarchical) model, we would need to introduce respondent-level heterogeneity into the data-generating process. In contrast to our current model, which assumes fixed preferences across all respondents, a hierarchical model allows each individual to have their own set of part-worth utilities.\n###Changes needed for simulation: - Instead of assigning a single () vector for all individuals, we would draw a unique (_i) for each respondent (i). - These respondent-specific (_i)s would be drawn from a population-level distribution, typically multivariate normal:\n[ _i (, ) ] - We would then simulate each respondent’s choices using their own (_i).\n###Changes needed for estimation: - We could no longer use standard MLE. Instead, we would use Bayesian hierarchical modeling or simulated maximum likelihood (e.g., using techniques like Hierarchical Bayes, Gibbs sampling, or variational inference). - The estimation process would recover both the population-level parameters ((), ()) and the individual-level (_i)s.\nThis approach is critical when analyzing real-world data, where individuals naturally vary in how they value price, brand, and other features. Multi-level models enable us to capture this variation and make more personalized predictions or segmentations."
  },
  {
    "objectID": "blogs/hw2/index.html",
    "href": "blogs/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nNow, let’s first read in the Blueprinty data.\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the data\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Quick look at the data\ndesc = blueprinty.drop(columns=[\"iscustomer\"]).describe()\ndesc_rounded = desc.copy()\ndesc_rounded.loc[\"mean\"] = desc_rounded.loc[\"mean\"].round(2)\ndesc_rounded.loc[\"std\"] = desc_rounded.loc[\"std\"].round(2)\ndesc_rounded.loc[~desc_rounded.index.isin([\"mean\", \"std\"])] = desc_rounded.loc[~desc_rounded.index.isin([\"mean\", \"std\"])].round(0)\ndesc_rounded.style\\\n    .format(\"{:.2f}\", subset=pd.IndexSlice[\"mean\", :])\\\n    .format(\"{:.2f}\", subset=pd.IndexSlice[\"std\", :])\\\n    .format(\"{:.0f}\", subset=pd.IndexSlice[[\"count\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"], :])\n\n\n\n\n\n\n\n \npatents\nage\n\n\n\n\ncount\n1500\n1500\n\n\nmean\n3.68\n26.36\n\n\nstd\n2.35\n7.24\n\n\nmin\n0\n9\n\n\n25%\n2\n21\n\n\n50%\n3\n26\n\n\n75%\n5\n32\n\n\nmax\n16\n49\n\n\n\n\n\nThe above table displays the statistic summary of numerical variables in the dataset.\nNext, a histogram of number of patens distribution by customer status bellow compares differences between customer groups.\n\n\nShow code\n# Set up the plot style\nsns.set(style=\"whitegrid\")\nmeans = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\npalette = {0: \"orange\", 1: \"green\"}\n# Histogram of number of patents by customer status\nplt.figure(figsize=(8, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", binwidth=1, palette=palette)\n\nplt.axvline(means[0], color=palette[0], linestyle=\"--\", linewidth=2, label=\"Mean (Non-customer)\")\nplt.axvline(means[1], color=palette[1], linestyle=\"--\", linewidth=2, label=\"Mean (Customer)\")\n\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom the histogram and mean values, it appears that customers of Blueprinty’s software tend to have more patents on average than non-customers. This may suggest that the software is associated with greater patenting success, but further modeling is necessary to control for confounding variables.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nAfter observing that Blueprinty customers might not be randomly selected, we explore whether there are systematic differences in region and firm age between customers and non-customers.\n\n\nShow code\n# Bar plot of region distribution by customer status\nblueprinty[\"iscustomer\"] = blueprinty[\"iscustomer\"].astype(str).astype(int)\nplt.figure(figsize=(8, 4))\nsns.countplot(data=blueprinty, x=\"region\", hue=\"iscustomer\", palette={0: \"orange\", 1: \"green\"})\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows that customer adoption varies by region. For instance, the Northeast has a disproportionately high number of customers, while the Southwest and Northwest are dominated by non-customers. This implies that regional variation could confound any relationship between software usage and patenting success, so it should be accounted for in modeling.\n\n\nShow code\n# Boxplot of age by customer status\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=blueprinty, x=\"iscustomer\", y=\"age\", hue=\"iscustomer\", palette={0: \"orange\", 1: \"green\"})\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status (0 = Non-customer, 1 = Customer)\")\nplt.ylabel(\"Age\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCustomers appear to be slightly older than non-customers, with a higher median and a broader age distribution. While the difference is modest, it still points to the possibility that more mature firms are more likely to adopt Blueprinty’s tools. Again, this reinforces the need to adjust for age when estimating treatment effects.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function of a Poisson distribution is:\n\\[\nP(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood for a sample of ( n ) observations is:\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood function is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nNow translate the log-likelihood expression into Python code. The function below takes a proposed value of λ and a vector of observed patent counts Y, and returns the total log-likelihood. We use gammaln(Y + 1) instead of log(Y!) to ensure numerical stability and avoid issues with large factorials.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"Compute the Poisson log-likelihood for a given lambda and data Y\"\"\"\n    if lambda_ &lt;= 0:\n        return -np.inf\n    loglik = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return loglik\n\n\nThis function will allow us to evaluate the fit of any given λ to the data. In the next step, we’ll search for the value of λ that maximizes this log-likelihood — our Maximum Likelihood Estimate (MLE).\n\n\nShow code\n# Define a range of lambda values to evaluate\nlambda_vals = np.linspace(0.1, 10, 200)\n\n# Compute log-likelihood for each lambda value\nloglik_vals = [poisson_loglikelihood(l, blueprinty[\"patents\"]) for l in lambda_vals]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color=\"purple\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood over a Range of λ\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s now find the Maximum Likelihood Estimate (MLE) for lambda by taking the derivative of the log-likelihood and setting it equal to zero:\n[ L() = {i=1}^{n} ( -1 + ) = -n + {i=1}^{n} Y_i ]\nSetting this equal to zero:\n[ -n + Y_i = 0 = Y_i = {Y} ]\nSo the MLE for lambda is simply the sample mean of the observed counts.\nAnd here’s a small Python code block to verify this numerically:\n\n\nShow code\nlambda_mle = blueprinty[\"patents\"].mean()\nlambda_mle.round(3)\n\n\n3.685\n\n\nWe can now find the Maximum Likelihood Estimate (MLE) for lambda numerically by maximizing the log-likelihood function. Since scipy.optimize performs minimization, we minimize the negative log-likelihood over a reasonable range of values for lambda.\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, blueprinty[\"patents\"]),\n    bounds=(0.1, 10),\n    method=\"bounded\"\n)\n\nlambda_mle_opt = result.x\nlambda_mle_opt.round(3)\n\n\n3.685\n\n\nThe numerical estimate of lambda (MLE) is 3.685, which matches the sample mean of the observed patent counts.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    loglik = np.sum(-lambda_ + Y * np.log(lambda_ + 1e-10) - gammaln(Y + 1))\n    return loglik\n\n\nWe prepare the covariate matrix X so that it has the structure needed for Poisson regression: a constant column, numeric variables, and encoded categorical variables.\n\n\nShow code\nimport pandas as pd\nimport numpy as np\n\n# Create dummy variables for region (drop one to avoid multicollinearity)\nblueprinty_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\n# Add age, age^2, customer, and intercept\nblueprinty[\"age2\"] = blueprinty[\"age\"] ** 2\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age2\", \"iscustomer\"]],\n    blueprinty_dummies\n], axis=1)\n\n# Convert to numpy array\nX_mat = X.values.astype(float)\nY = blueprinty[\"patents\"].values.astype(float)\n\n\nWe use scipy.optimize.minimize with the BFGS method to find the MLEs of our regression coefficients.\n\n\nShow code\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood function\nneg_loglik = lambda beta: -poisson_regression_loglikelihood(beta, Y, X_mat)\n\nbeta_init = np.full(X_mat.shape[1], -0.1)\n\n# Minimize it\nresult = minimize(neg_loglik, beta_init, method=\"BFGS\")\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n\nThe table below reports the estimated effect of each variable on the number of patents a firm receives, along with the standard error of each estimate.\n\n\nShow code\n# Standard errors from the inverse Hessian\nse = np.sqrt(np.diag(hessian_inv))\n# Combine into a tidy table\ncoef_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se\n})\ncoef_table.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n\n\n\n\n0\nintercept\n5.7853\n1.0\n\n\n1\nage\n77.2949\n1.0\n\n\n2\nage2\n1031.2168\n1.0\n\n\n3\niscustomer\n2.0352\n1.0\n\n\n4\nNortheast\n2.5411\n1.0\n\n\n5\nNorthwest\n0.6923\n1.0\n\n\n6\nSouth\n0.4914\n1.0\n\n\n7\nSouthwest\n0.9087\n1.0\n\n\n\n\n\n\n\nTo confirm the accuracy of our hand-coded Poisson regression, we also estimate the model using statsmodels.GLM with the Poisson family. The table below presents the coefficient estimates, standard errors, z-scores, and confidence intervals.\n\n\nShow code\nimport statsmodels.api as sm\n\n# Fit the Poisson regression using GLM\nglm_model = sm.GLM(Y, X_mat, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display results in a clean table\nglm_summary = glm_results.summary2().tables[1].reset_index().rename(columns={\"index\": \"Variable\"})\nglm_summary.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nx1\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n2\nx2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n3\nx3\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n4\nx4\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n5\nx5\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n6\nx6\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n7\nx7\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\n\n\n\n\nThe table above summarizes the results from our Poisson regression model using statsmodels. Here’s what we observe:\n\nx1 (age): The coefficient is positive and highly significant (p&lt;0.001), indicating that older firms tend to receive more patents, all else equal.\nx2 (age squared): The coefficient is small, negative, and also highly significant. This suggests a concave relationship — the positive effect of age diminishes at higher ages, meaning the relationship between age and patents is likely hump-shaped.\nx3 (iscustomer): The customer indicator is positive and significant. This implies that firms who use Blueprinty’s software are associated with higher patent counts, controlling for other factors. This is consistent with the hypothesis that the software may help firms better navigate the patent process.\nx4 to x7 (region dummies): enderNone of the region dummy variables are statistically significant at conventional levels (p&gt;0.05). This suggests that after controlling for age and customer status, the region of a firm does not meaningfully affect patent count in this sample.\nIntercept (const): The baseline level of patenting (for a non-customer firm with age zero in the reference region) is negative, as expected — but not of direct substantive interest.\n\nDue to the nonlinear nature of the Poisson regression model, the estimated coefficients—particularly for binary variables like iscustomer—are not directly interpretable in terms of marginal effects on the outcome variable. That is, the coefficient on iscustomer does not represent a constant additive change in the expected number of patents.\nTo obtain a more interpretable estimate of the effect of using Blueprinty’s software, we implement a counterfactual prediction approach. Specifically, we construct two hypothetical datasets:\n\nScenario 1 (X₀): Each firm is treated as a non-customer (iscustomer = 0)\nScenario 2 (X₁): Each firm is treated as a customer (iscustomer = 1)\n\nAll other covariates are held constant. We then use our fitted Poisson model to generate predicted patent counts for each firm under both scenarios. The average difference between the predicted outcomes under X₁ and X₀ represents the estimated effect of Blueprinty’s software on patenting activity.\nBased on this approach, we estimate that firms using Blueprinty’s software produce, on average, 0.79 more patents than they would have without the software, holding all else equal. This provides evidence consistent with the hypothesis that the software enhances patenting success."
  },
  {
    "objectID": "blogs/hw2/index.html#blueprinty-case-study",
    "href": "blogs/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nNow, let’s first read in the Blueprinty data.\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the data\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Quick look at the data\ndesc = blueprinty.drop(columns=[\"iscustomer\"]).describe()\ndesc_rounded = desc.copy()\ndesc_rounded.loc[\"mean\"] = desc_rounded.loc[\"mean\"].round(2)\ndesc_rounded.loc[\"std\"] = desc_rounded.loc[\"std\"].round(2)\ndesc_rounded.loc[~desc_rounded.index.isin([\"mean\", \"std\"])] = desc_rounded.loc[~desc_rounded.index.isin([\"mean\", \"std\"])].round(0)\ndesc_rounded.style\\\n    .format(\"{:.2f}\", subset=pd.IndexSlice[\"mean\", :])\\\n    .format(\"{:.2f}\", subset=pd.IndexSlice[\"std\", :])\\\n    .format(\"{:.0f}\", subset=pd.IndexSlice[[\"count\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"], :])\n\n\n\n\n\n\n\n \npatents\nage\n\n\n\n\ncount\n1500\n1500\n\n\nmean\n3.68\n26.36\n\n\nstd\n2.35\n7.24\n\n\nmin\n0\n9\n\n\n25%\n2\n21\n\n\n50%\n3\n26\n\n\n75%\n5\n32\n\n\nmax\n16\n49\n\n\n\n\n\nThe above table displays the statistic summary of numerical variables in the dataset.\nNext, a histogram of number of patens distribution by customer status bellow compares differences between customer groups.\n\n\nShow code\n# Set up the plot style\nsns.set(style=\"whitegrid\")\nmeans = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\npalette = {0: \"orange\", 1: \"green\"}\n# Histogram of number of patents by customer status\nplt.figure(figsize=(8, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", binwidth=1, palette=palette)\n\nplt.axvline(means[0], color=palette[0], linestyle=\"--\", linewidth=2, label=\"Mean (Non-customer)\")\nplt.axvline(means[1], color=palette[1], linestyle=\"--\", linewidth=2, label=\"Mean (Customer)\")\n\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom the histogram and mean values, it appears that customers of Blueprinty’s software tend to have more patents on average than non-customers. This may suggest that the software is associated with greater patenting success, but further modeling is necessary to control for confounding variables.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nAfter observing that Blueprinty customers might not be randomly selected, we explore whether there are systematic differences in region and firm age between customers and non-customers.\n\n\nShow code\n# Bar plot of region distribution by customer status\nblueprinty[\"iscustomer\"] = blueprinty[\"iscustomer\"].astype(str).astype(int)\nplt.figure(figsize=(8, 4))\nsns.countplot(data=blueprinty, x=\"region\", hue=\"iscustomer\", palette={0: \"orange\", 1: \"green\"})\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows that customer adoption varies by region. For instance, the Northeast has a disproportionately high number of customers, while the Southwest and Northwest are dominated by non-customers. This implies that regional variation could confound any relationship between software usage and patenting success, so it should be accounted for in modeling.\n\n\nShow code\n# Boxplot of age by customer status\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=blueprinty, x=\"iscustomer\", y=\"age\", hue=\"iscustomer\", palette={0: \"orange\", 1: \"green\"})\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status (0 = Non-customer, 1 = Customer)\")\nplt.ylabel(\"Age\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCustomers appear to be slightly older than non-customers, with a higher median and a broader age distribution. While the difference is modest, it still points to the possibility that more mature firms are more likely to adopt Blueprinty’s tools. Again, this reinforces the need to adjust for age when estimating treatment effects.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function of a Poisson distribution is:\n\\[\nP(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood for a sample of ( n ) observations is:\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood function is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nNow translate the log-likelihood expression into Python code. The function below takes a proposed value of λ and a vector of observed patent counts Y, and returns the total log-likelihood. We use gammaln(Y + 1) instead of log(Y!) to ensure numerical stability and avoid issues with large factorials.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    \"\"\"Compute the Poisson log-likelihood for a given lambda and data Y\"\"\"\n    if lambda_ &lt;= 0:\n        return -np.inf\n    loglik = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n    return loglik\n\n\nThis function will allow us to evaluate the fit of any given λ to the data. In the next step, we’ll search for the value of λ that maximizes this log-likelihood — our Maximum Likelihood Estimate (MLE).\n\n\nShow code\n# Define a range of lambda values to evaluate\nlambda_vals = np.linspace(0.1, 10, 200)\n\n# Compute log-likelihood for each lambda value\nloglik_vals = [poisson_loglikelihood(l, blueprinty[\"patents\"]) for l in lambda_vals]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color=\"purple\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood over a Range of λ\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s now find the Maximum Likelihood Estimate (MLE) for lambda by taking the derivative of the log-likelihood and setting it equal to zero:\n[ L() = {i=1}^{n} ( -1 + ) = -n + {i=1}^{n} Y_i ]\nSetting this equal to zero:\n[ -n + Y_i = 0 = Y_i = {Y} ]\nSo the MLE for lambda is simply the sample mean of the observed counts.\nAnd here’s a small Python code block to verify this numerically:\n\n\nShow code\nlambda_mle = blueprinty[\"patents\"].mean()\nlambda_mle.round(3)\n\n\n3.685\n\n\nWe can now find the Maximum Likelihood Estimate (MLE) for lambda numerically by maximizing the log-likelihood function. Since scipy.optimize performs minimization, we minimize the negative log-likelihood over a reasonable range of values for lambda.\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, blueprinty[\"patents\"]),\n    bounds=(0.1, 10),\n    method=\"bounded\"\n)\n\nlambda_mle_opt = result.x\nlambda_mle_opt.round(3)\n\n\n3.685\n\n\nThe numerical estimate of lambda (MLE) is 3.685, which matches the sample mean of the observed patent counts.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    loglik = np.sum(-lambda_ + Y * np.log(lambda_ + 1e-10) - gammaln(Y + 1))\n    return loglik\n\n\nWe prepare the covariate matrix X so that it has the structure needed for Poisson regression: a constant column, numeric variables, and encoded categorical variables.\n\n\nShow code\nimport pandas as pd\nimport numpy as np\n\n# Create dummy variables for region (drop one to avoid multicollinearity)\nblueprinty_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\n# Add age, age^2, customer, and intercept\nblueprinty[\"age2\"] = blueprinty[\"age\"] ** 2\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age2\", \"iscustomer\"]],\n    blueprinty_dummies\n], axis=1)\n\n# Convert to numpy array\nX_mat = X.values.astype(float)\nY = blueprinty[\"patents\"].values.astype(float)\n\n\nWe use scipy.optimize.minimize with the BFGS method to find the MLEs of our regression coefficients.\n\n\nShow code\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood function\nneg_loglik = lambda beta: -poisson_regression_loglikelihood(beta, Y, X_mat)\n\nbeta_init = np.full(X_mat.shape[1], -0.1)\n\n# Minimize it\nresult = minimize(neg_loglik, beta_init, method=\"BFGS\")\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n\nThe table below reports the estimated effect of each variable on the number of patents a firm receives, along with the standard error of each estimate.\n\n\nShow code\n# Standard errors from the inverse Hessian\nse = np.sqrt(np.diag(hessian_inv))\n# Combine into a tidy table\ncoef_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se\n})\ncoef_table.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n\n\n\n\n0\nintercept\n5.7853\n1.0\n\n\n1\nage\n77.2949\n1.0\n\n\n2\nage2\n1031.2168\n1.0\n\n\n3\niscustomer\n2.0352\n1.0\n\n\n4\nNortheast\n2.5411\n1.0\n\n\n5\nNorthwest\n0.6923\n1.0\n\n\n6\nSouth\n0.4914\n1.0\n\n\n7\nSouthwest\n0.9087\n1.0\n\n\n\n\n\n\n\nTo confirm the accuracy of our hand-coded Poisson regression, we also estimate the model using statsmodels.GLM with the Poisson family. The table below presents the coefficient estimates, standard errors, z-scores, and confidence intervals.\n\n\nShow code\nimport statsmodels.api as sm\n\n# Fit the Poisson regression using GLM\nglm_model = sm.GLM(Y, X_mat, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display results in a clean table\nglm_summary = glm_results.summary2().tables[1].reset_index().rename(columns={\"index\": \"Variable\"})\nglm_summary.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nx1\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n2\nx2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n3\nx3\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n4\nx4\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n5\nx5\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n6\nx6\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n7\nx7\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\n\n\n\n\nThe table above summarizes the results from our Poisson regression model using statsmodels. Here’s what we observe:\n\nx1 (age): The coefficient is positive and highly significant (p&lt;0.001), indicating that older firms tend to receive more patents, all else equal.\nx2 (age squared): The coefficient is small, negative, and also highly significant. This suggests a concave relationship — the positive effect of age diminishes at higher ages, meaning the relationship between age and patents is likely hump-shaped.\nx3 (iscustomer): The customer indicator is positive and significant. This implies that firms who use Blueprinty’s software are associated with higher patent counts, controlling for other factors. This is consistent with the hypothesis that the software may help firms better navigate the patent process.\nx4 to x7 (region dummies): enderNone of the region dummy variables are statistically significant at conventional levels (p&gt;0.05). This suggests that after controlling for age and customer status, the region of a firm does not meaningfully affect patent count in this sample.\nIntercept (const): The baseline level of patenting (for a non-customer firm with age zero in the reference region) is negative, as expected — but not of direct substantive interest.\n\nDue to the nonlinear nature of the Poisson regression model, the estimated coefficients—particularly for binary variables like iscustomer—are not directly interpretable in terms of marginal effects on the outcome variable. That is, the coefficient on iscustomer does not represent a constant additive change in the expected number of patents.\nTo obtain a more interpretable estimate of the effect of using Blueprinty’s software, we implement a counterfactual prediction approach. Specifically, we construct two hypothetical datasets:\n\nScenario 1 (X₀): Each firm is treated as a non-customer (iscustomer = 0)\nScenario 2 (X₁): Each firm is treated as a customer (iscustomer = 1)\n\nAll other covariates are held constant. We then use our fitted Poisson model to generate predicted patent counts for each firm under both scenarios. The average difference between the predicted outcomes under X₁ and X₀ represents the estimated effect of Blueprinty’s software on patenting activity.\nBased on this approach, we estimate that firms using Blueprinty’s software produce, on average, 0.79 more patents than they would have without the software, holding all else equal. This provides evidence consistent with the hypothesis that the software enhances patenting success."
  },
  {
    "objectID": "blogs/hw2/index.html#airbnb-case-study",
    "href": "blogs/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\nLoad and Clean Data\nWe begin by loading the AirBnB dataset, which is shown as below. The goal is to understand what factors are associated with a higher number of reviews, which we treat as a proxy for booking activity.\n\n\nShow code\nimport pandas as pd\n\n# Load the dataset\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n\nBelow is a summary of the dataset’s structure and variable distributions.\n\n\nShow code\nairbnb.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nUnnamed: 0\n40628.0\n2.031450e+04\n1.172844e+04\n1.0\n10157.75\n20314.5\n30471.25\n40628.0\n\n\nid\n40628.0\n9.698889e+06\n5.460166e+06\n2515.0\n4889868.25\n9862877.5\n14667893.75\n18009669.0\n\n\ndays\n40628.0\n1.102368e+03\n1.383269e+03\n1.0\n542.00\n996.0\n1535.00\n42828.0\n\n\nbathrooms\n40468.0\n1.124592e+00\n3.858843e-01\n0.0\n1.00\n1.0\n1.00\n8.0\n\n\nbedrooms\n40552.0\n1.147046e+00\n6.917461e-01\n0.0\n1.00\n1.0\n1.00\n10.0\n\n\nprice\n40628.0\n1.447607e+02\n2.106576e+02\n10.0\n70.00\n100.0\n170.00\n10000.0\n\n\nnumber_of_reviews\n40628.0\n1.590443e+01\n2.924601e+01\n0.0\n1.00\n4.0\n17.00\n421.0\n\n\nreview_scores_cleanliness\n30433.0\n9.198370e+00\n1.119935e+00\n2.0\n9.00\n10.0\n10.00\n10.0\n\n\nreview_scores_location\n30374.0\n9.413544e+00\n8.449491e-01\n2.0\n9.00\n10.0\n10.00\n10.0\n\n\nreview_scores_value\n30372.0\n9.331522e+00\n9.029656e-01\n2.0\n9.00\n10.0\n10.00\n10.0\n\n\n\n\n\n\n\nSeveral variables in the dataset contain missing values. Notably, over 10,000 listings are missing review scores related to value, location, and cleanliness—likely because those listings received no reviews. Since our outcome of interest is the number of reviews, we can reasonably restrict our analysis to listings with at least one review. Additionally, we drop a small number of rows with missing data in key listing features such as bathrooms, bedrooms, and host listing date. Here’s the code to filter and clean accordingly:\n\n\nShow code\n# Drop listings with zero reviews\nairbnb_filtered = airbnb[airbnb[\"number_of_reviews\"] &gt; 0].copy()\n\n# Drop rows with missing values in relevant predictors\ncols_to_check = [\"bathrooms\", \"bedrooms\", \"host_since\",\n                 \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"]\nairbnb_filtered = airbnb_filtered.dropna(subset=cols_to_check)\n\n\n\n\nExploratory Data Analysis\nTo begin, we examine the distributions of key variables:\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\nsns.histplot(airbnb_filtered[\"number_of_reviews\"], bins=50, color=\"green\", kde=False, ax=axes[0, 0])\naxes[0, 0].set_title(\"Number of Reviews\")\n\nsns.histplot(airbnb_filtered[\"price\"], bins=50, color=\"green\", kde=False, ax=axes[0, 1])\naxes[0, 1].set_title(\"Price per Night\")\n\nsns.histplot(airbnb_filtered[\"review_scores_cleanliness\"], bins=10, color=\"green\", kde=False, ax=axes[1, 0])\naxes[1, 0].set_title(\"Cleanliness Score\")\n\nsns.histplot(airbnb_filtered[\"review_scores_value\"], bins=10, color=\"green\", kde=False, ax=axes[1, 1])\naxes[1, 1].set_title(\"Value Score\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNumber of Reviews: Highly right-skewed. Most listings receive a modest number of reviews, but a small number receive over 100. This validates the use of a count-based model such as Poisson regression.\nPrice per Night: Also right-skewed, with the majority of listings priced under $500. A few outliers appear above $1,000.\nReview Scores (Cleanliness & Value): Both are tightly clustered near the upper end of the scale (around 9–10), suggesting generally favorable customer feedback with limited variation.\nThese patterns support the idea that modeling review counts (as a proxy for bookings) using a Poisson regression is appropriate, provided we carefully control for influential predictors like price and review quality.\n\n\nPoisson Regression: Predicting Number of Reviews\nWe fit a Poisson regression model to predict the number of reviews (used as a proxy for bookings) based on a selection of listing characteristics.\n\n\nShow code\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Select predictors\npredictors = [\n    \"price\",\n    \"review_scores_cleanliness\",\n    \"review_scores_location\",\n    \"review_scores_value\",\n    \"bedrooms\",\n    \"bathrooms\"\n]\n\n# Drop rows with missing data\nairbnb_model_data = airbnb_filtered.dropna(subset=predictors + [\"number_of_reviews\"]).copy()\n\n# Create design matrix\nX = sm.add_constant(airbnb_model_data[predictors])\nY = airbnb_model_data[\"number_of_reviews\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Display summary\nsummary_table = poisson_model.summary2().tables[1].reset_index().rename(columns={\"index\": \"Variable\"})\nsummary_table.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n3.7035\n0.0157\n236.5826\n0.0000\n3.6729\n3.7342\n\n\n1\nprice\n-0.0000\n0.0000\n-3.1641\n0.0016\n-0.0000\n-0.0000\n\n\n2\nreview_scores_cleanliness\n0.1138\n0.0015\n76.6885\n0.0000\n0.1109\n0.1167\n\n\n3\nreview_scores_location\n-0.0803\n0.0016\n-50.3224\n0.0000\n-0.0834\n-0.0771\n\n\n4\nreview_scores_value\n-0.0968\n0.0018\n-54.2270\n0.0000\n-0.1003\n-0.0933\n\n\n5\nbedrooms\n0.0770\n0.0020\n38.8908\n0.0000\n0.0731\n0.0809\n\n\n6\nbathrooms\n-0.1179\n0.0038\n-31.3070\n0.0000\n-0.1253\n-0.1106\n\n\n\n\n\n\n\n\nIntercept: Represents the expected log number of reviews when all predictors are at zero.\nPrice: Small, negative, and statistically significant — more expensive listings receive slightly fewer reviews, on average.\nReview Scores – Cleanliness: Positive and highly significant. Cleanliness appears to drive guest engagement.\nReview Scores – Location & Value: Both are negative and significant, which may seem surprising. This could reflect multicollinearity or indicate that higher ratings correspond with fewer, but higher-quality bookings.\n\nAll coefficients are statistically significant at the 0.001 level, suggesting meaningful associations between listing characteristics and review frequency.\n\n\nModel Fit: Predicted vs. Actual Reviews\n\n\nShow code\nairbnb_model_data[\"predicted_reviews\"] = poisson_model.predict(X)\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(\n    x=airbnb_model_data[\"number_of_reviews\"],\n    y=airbnb_model_data[\"predicted_reviews\"],\n    alpha=0.5, \n    color=\"green\"\n)\nplt.xlabel(\"Actual Number of Reviews\")\nplt.ylabel(\"Predicted Number of Reviews\")\nplt.title(\"Predicted vs. Actual Number of Reviews (Poisson Model)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nComparing predicted review counts from the Poisson model to the actual observed counts for each listing, the plot shows a strong positive relationship between actual and predicted values, especially for listings with fewer than 100 reviews. There is some deviation for listings with very high review counts, suggesting possible overdispersion—a common issue in count data where variance exceeds the mean. Despite this, the model captures the general trend well and provides a solid baseline for understanding how listing characteristics relate to booking activity."
  },
  {
    "objectID": "blogs/hw4/index.html",
    "href": "blogs/hw4/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "blogs/hw4/index.html#a.-k-means",
    "href": "blogs/hw4/index.html#a.-k-means",
    "title": "Machine Learning",
    "section": "1a. K-Means",
    "text": "1a. K-Means\n\nLoad and Prepare the Data\nWe start by loading the Palmer Penguins dataset and selecting only the two numeric features we’ll use for clustering: bill length and flipper length. We also scale the data to have zero mean and unit variance.\n\n\nShow code\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data and select features\ndf = pd.read_csv(\"palmer_penguins.csv\")\ndf = df[['bill_length_mm', 'flipper_length_mm']].dropna()\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(df)\n\n\n\n\nDefine Helper Functions\nTo build our own K-Means algorithm, we need a function to compute Euclidean distances between points and cluster centroids.\n\n\nShow code\n# Helper: Euclidean distance between array of points and a centroid\ndef euclidean(a, b):\n    return np.linalg.norm(a - b, axis=1)\n\n\n\n\nImplement K-Means from Scratch\nHere’s the main loop of the K-Means algorithm: 1. Randomly initialize k centroids. 2. Assign each point to the nearest centroid. 3. Recompute the centroids. 4. Repeat until convergence.\nWe also keep track of centroid history for visualization.\n\n\nShow code\ndef kmeans_custom(X, k=3, max_iters=10, seed=42):\n    np.random.seed(seed)\n    centroids = X[np.random.choice(len(X), size=k, replace=False)]\n    history = [centroids.copy()]\n\n    for _ in range(max_iters):\n        distances = np.array([euclidean(X, c) for c in centroids])\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n        \n        history.append(new_centroids.copy())\n        if np.allclose(new_centroids, centroids):\n            break\n        centroids = new_centroids\n    \n    return labels, centroids, history\n\n\n\n\nRun the Algorithm\nLet’s run the custom K-Means on the penguins dataset with k=3 and visualize the result.\n\n\nShow code\nlabels, final_centroids, history = kmeans_custom(X, k=3)\n\n\n\n\nVisualize Final Clustering\nWe plot the data points colored by cluster assignment, along with the final centroids.\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\nplt.scatter(final_centroids[:, 0], final_centroids[:, 1], color='red', marker='X', s=200)\nplt.xlabel(\"Bill Length (scaled)\")\nplt.ylabel(\"Flipper Length (scaled)\")\nplt.title(\"Custom K-Means Clustering (k=3)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot illustrates the result of applying a custom K-Means clustering algorithm with k=3 to the Palmer Penguins dataset, using bill length and flipper length as input features. Each point represents a penguin, colored by its assigned cluster, and the large red Xs indicate the final centroid of each cluster. The clustering reveals three distinct groups: one consisting of penguins with relatively shorter bills and flippers (left cluster), another with longer bills but shorter flippers (bottom-right), and a third with both longer bills and flippers (top-right). The clear separation and compactness of the clusters suggest that bill and flipper length are useful features for grouping the penguins, potentially reflecting differences in species or ecological niches. Since the features were standardized, the interpretation focuses on relative size rather than raw measurements.\n\n\nEvaluate the Optimal Number of Clusters\nTo assess the best value for ( k ), we compute two metrics:\n\nWithin-Cluster Sum of Squares (WCSS): Measures compactness; lower is better.\nSilhouette Score: Measures how well-separated the clusters are; ranges from -1 to 1, and higher is better.\n\nWe compute both for ( k = 2 ) to ( 7 ) using the built-in KMeans implementation from sklearn.\n\n\nShow code\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette_scores = []\nk_values = range(2, 8)\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X)\n    \n    wcss.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, labels))\n\n\n\n\nPlot the Metrics\nWe now visualize the WCSS and silhouette scores to identify the “best” number of clusters.\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# WCSS (Elbow method)\nax[0].plot(k_values, wcss, marker='o')\nax[0].set_title('Elbow Method (WCSS)')\nax[0].set_xlabel('Number of clusters (k)')\nax[0].set_ylabel('WCSS')\n\n# Silhouette Scores\nax[1].plot(k_values, silhouette_scores, marker='o', color='green')\nax[1].set_title('Silhouette Score')\nax[1].set_xlabel('Number of clusters (k)')\nax[1].set_ylabel('Score')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe Elbow Method plot displays the Within-Cluster Sum of Squares (WCSS) for different values of k ranging from 2 to 7. WCSS measures the compactness of the clusters, with lower values indicating tighter, more cohesive clusters. As the number of clusters increases, WCSS naturally decreases. However, the key insight comes from identifying the “elbow” point—where the rate of decrease sharply levels off. In this plot, the most noticeable elbow appears at k=3, suggesting that increasing the number of clusters beyond 3 results in only marginal improvements in cluster compactness. Therefore, k=3 is likely the optimal number of clusters, balancing simplicity with good fit."
  },
  {
    "objectID": "blogs/hw4/index.html#b.-latent-class-mnl",
    "href": "blogs/hw4/index.html#b.-latent-class-mnl",
    "title": "Machine Learning",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "blogs/hw4/index.html#a.-k-nearest-neighbors",
    "href": "blogs/hw4/index.html#a.-k-nearest-neighbors",
    "title": "Machine Learning",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\n\n# gen data\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nx = np.column_stack((x1, x2))\n\n# define a wiggly boundary\nboundary = np.sin(4 * x1) + x1\ny = np.where(x2 &gt; boundary, 1, 0).astype(str)  # convert to string to mimic factor\ndat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "blogs/hw4/index.html#b.-key-drivers-analysis",
    "href": "blogs/hw4/index.html#b.-key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "blogs/hw1/index.html",
    "href": "blogs/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis study investigate whether the presence and size of a matching grant—commonly used by nonprofits—significantly affects donor behavior. The matching grant acts as a potential price reduction for donating, which may alter individuals’ willingness to give. The experiment’s large sample and randomized design make it especially well-suited to test economic theories of altruism, incentives, and social signaling.\nThis project seeks to replicate the core findings of Karlan and List (2007), using the original dataset provided by the authors. Through the replication, I aim to confirm their main results, including how matching grants affect both the likelihood of donating and the average amount donated. I also explore potential heterogeneity in treatment effects by political geography, as highlighted in the original paper."
  },
  {
    "objectID": "blogs/hw1/index.html#introduction",
    "href": "blogs/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis study investigate whether the presence and size of a matching grant—commonly used by nonprofits—significantly affects donor behavior. The matching grant acts as a potential price reduction for donating, which may alter individuals’ willingness to give. The experiment’s large sample and randomized design make it especially well-suited to test economic theories of altruism, incentives, and social signaling.\nThis project seeks to replicate the core findings of Karlan and List (2007), using the original dataset provided by the authors. Through the replication, I aim to confirm their main results, including how matching grants affect both the likelihood of donating and the average amount donated. I also explore potential heterogeneity in treatment effects by political geography, as highlighted in the original paper."
  },
  {
    "objectID": "blogs/hw1/index.html#data",
    "href": "blogs/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis dataset comes from a large-scale natural field experiment conducted by Karlan and List (2007) to study the impact of different fundraising strategies on charitable donations. It contains data from over 50,000 past donors to a U.S.-based nonprofit organization who were randomly assigned to receive one of several types of solicitation letters.\nEach observation in the dataset corresponds to a single individual donor. The dataset includes variables that capture:\n- Treatment assignment including whether the donor received a matching grant offer and the specific match ratio (1:1, 2:1, or 3:1).\n- Suggested donation amounts which were manipulated as part of the experimental design.\n- Donation behavior such as whether the person donated in response, and how much they gave.\n- Demographic indicators and prior donation history (e.g., years since first gift, total number of past donations, gender).\n- Geographic and political indicators such as whether the donor lived in a “red” or “blue” state during the 2004 presidential election, and the level of activity of the organization in that state.\nThe following Python code chunk loads the original dataset Dean Karlan used:\n\n\nShow code\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n\n\n\nData Overview\nThis following table provides an overview of all variables in the dataset. It includes each variable’s name, its data type (e.g., numeric or categorical), and the number of missing values.\n\n\nShow code\noverview_table = pd.DataFrame({\n    \"Variable\": df.columns,\n    \"Data Type\": df.dtypes.values,\n    \"Missing Values\": df.isnull().sum().values\n})\n\noverview_table\n\n\n\n\n\n\n\n\n\nVariable\nData Type\nMissing Values\n\n\n\n\n0\ntreatment\nint8\n0\n\n\n1\ncontrol\nint8\n0\n\n\n2\nratio\ncategory\n0\n\n\n3\nratio2\nint8\n0\n\n\n4\nratio3\nint8\n0\n\n\n5\nsize\ncategory\n0\n\n\n6\nsize25\nint8\n0\n\n\n7\nsize50\nint8\n0\n\n\n8\nsize100\nint8\n0\n\n\n9\nsizeno\nint8\n0\n\n\n10\nask\ncategory\n0\n\n\n11\naskd1\nint8\n0\n\n\n12\naskd2\nint8\n0\n\n\n13\naskd3\nint8\n0\n\n\n14\nask1\nint16\n0\n\n\n15\nask2\nint16\n0\n\n\n16\nask3\nint16\n0\n\n\n17\namount\nfloat32\n0\n\n\n18\ngave\nint8\n0\n\n\n19\namountchange\nfloat32\n0\n\n\n20\nhpa\nfloat32\n0\n\n\n21\nltmedmra\nint8\n0\n\n\n22\nfreq\nint16\n0\n\n\n23\nyears\nfloat64\n1\n\n\n24\nyear5\nint8\n0\n\n\n25\nmrm2\nfloat64\n1\n\n\n26\ndormant\nint8\n0\n\n\n27\nfemale\nfloat64\n1111\n\n\n28\ncouple\nfloat64\n1148\n\n\n29\nstate50one\nint8\n0\n\n\n30\nnonlit\nfloat64\n452\n\n\n31\ncases\nfloat64\n452\n\n\n32\nstatecnt\nfloat32\n0\n\n\n33\nstateresponse\nfloat32\n0\n\n\n34\nstateresponset\nfloat32\n0\n\n\n35\nstateresponsec\nfloat32\n3\n\n\n36\nstateresponsetminc\nfloat32\n3\n\n\n37\nperbush\nfloat32\n35\n\n\n38\nclose25\nfloat64\n35\n\n\n39\nred0\nfloat64\n35\n\n\n40\nblue0\nfloat64\n35\n\n\n41\nredcty\nfloat64\n105\n\n\n42\nbluecty\nfloat64\n105\n\n\n43\npwhite\nfloat32\n1866\n\n\n44\npblack\nfloat32\n2036\n\n\n45\npage18_39\nfloat32\n1866\n\n\n46\nave_hh_sz\nfloat32\n1862\n\n\n47\nmedian_hhincome\nfloat64\n1874\n\n\n48\npowner\nfloat32\n1869\n\n\n49\npsch_atlstba\nfloat32\n1868\n\n\n50\npop_propurban\nfloat32\n1866\n\n\n\n\n\n\n\n\n\nVariable Definitions\nThe summary statistics table below presents descriptive metrics for all numeric variables in this dataset, including count, mean, standard deviation, and range (min/max).\n\n\nShow code\ndf.describe().transpose().round(2)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntreatment\n50083.0\n0.67\n0.47\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\ncontrol\n50083.0\n0.33\n0.47\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\nratio2\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nratio3\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nsize25\n50083.0\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nsize50\n50083.0\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nsize100\n50083.0\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nsizeno\n50083.0\n0.17\n0.37\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\naskd1\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\naskd2\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\naskd3\n50083.0\n0.22\n0.42\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nask1\n50083.0\n71.50\n101.73\n25.00\n35.00\n45.00\n65.00\n1500.00\n\n\nask2\n50083.0\n91.79\n127.25\n35.00\n45.00\n60.00\n85.00\n1875.00\n\n\nask3\n50083.0\n111.05\n151.67\n50.00\n55.00\n70.00\n100.00\n2250.00\n\n\namount\n50083.0\n0.92\n8.71\n0.00\n0.00\n0.00\n0.00\n400.00\n\n\ngave\n50083.0\n0.02\n0.14\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\namountchange\n50083.0\n-52.67\n1267.10\n-200412.12\n-50.00\n-30.00\n-25.00\n275.00\n\n\nhpa\n50083.0\n59.38\n71.18\n0.00\n30.00\n45.00\n60.00\n1000.00\n\n\nltmedmra\n50083.0\n0.49\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\nfreq\n50083.0\n8.04\n11.39\n0.00\n2.00\n4.00\n10.00\n218.00\n\n\nyears\n50082.0\n6.10\n5.50\n0.00\n2.00\n5.00\n9.00\n95.00\n\n\nyear5\n50083.0\n0.51\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\nmrm2\n50082.0\n13.01\n12.08\n0.00\n4.00\n8.00\n19.00\n168.00\n\n\ndormant\n50083.0\n0.52\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\nfemale\n48972.0\n0.28\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\ncouple\n48935.0\n0.09\n0.29\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nstate50one\n50083.0\n0.00\n0.03\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nnonlit\n49631.0\n2.47\n1.96\n0.00\n1.00\n3.00\n4.00\n6.00\n\n\ncases\n49631.0\n1.50\n1.16\n0.00\n1.00\n1.00\n2.00\n4.00\n\n\nstatecnt\n50083.0\n6.00\n5.75\n0.00\n1.83\n3.54\n9.61\n17.37\n\n\nstateresponse\n50083.0\n0.02\n0.01\n0.00\n0.02\n0.02\n0.02\n0.08\n\n\nstateresponset\n50083.0\n0.02\n0.01\n0.00\n0.02\n0.02\n0.02\n0.11\n\n\nstateresponsec\n50080.0\n0.02\n0.01\n0.00\n0.01\n0.02\n0.02\n0.05\n\n\nstateresponsetminc\n50080.0\n0.00\n0.01\n-0.05\n-0.00\n0.00\n0.01\n0.11\n\n\nperbush\n50048.0\n0.49\n0.08\n0.09\n0.44\n0.48\n0.53\n0.73\n\n\nclose25\n50048.0\n0.19\n0.39\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nred0\n50048.0\n0.40\n0.49\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\nblue0\n50048.0\n0.60\n0.49\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\nredcty\n49978.0\n0.51\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n\n\nbluecty\n49978.0\n0.49\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n\n\npwhite\n48217.0\n0.82\n0.17\n0.01\n0.76\n0.87\n0.94\n1.00\n\n\npblack\n48047.0\n0.09\n0.14\n0.00\n0.01\n0.04\n0.09\n0.99\n\n\npage18_39\n48217.0\n0.32\n0.10\n0.00\n0.26\n0.31\n0.37\n1.00\n\n\nave_hh_sz\n48221.0\n2.43\n0.38\n0.00\n2.21\n2.44\n2.66\n5.27\n\n\nmedian_hhincome\n48209.0\n54815.70\n22027.32\n5000.00\n39181.00\n50673.00\n66005.00\n200001.00\n\n\npowner\n48214.0\n0.67\n0.19\n0.00\n0.56\n0.71\n0.82\n1.00\n\n\npsch_atlstba\n48215.0\n0.39\n0.19\n0.00\n0.24\n0.37\n0.53\n1.00\n\n\npop_propurban\n48217.0\n0.87\n0.26\n0.00\n0.88\n1.00\n1.00\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\n\ntreated = df[df['treatment'] == 1]\ncontrol = df[df['treatment'] == 0]\n\ntable1_vars = [\n    'mrm2',    \n    'hpa',      \n    'freq',      \n    'years',     \n    'year5',     \n    'female', 'couple',\n    'redcty',\n    'nonlit', 'cases'\n]\n\ndef manual_ttest(var):\n    Xt, Xc = treated[var], control[var]\n    mean_diff = Xt.mean() - Xc.mean()\n    pooled_se = np.sqrt(Xt.var(ddof=1)/len(Xt) + Xc.var(ddof=1)/len(Xc))\n    t_stat = mean_diff / pooled_se\n    return {\n        \"Variable\": var,\n        \"Mean_Treatment\": round(Xt.mean(), 3),\n        \"Mean_Control\": round(Xc.mean(), 3),\n        \"t-stat\": round(t_stat, 4)\n    }\n\nmanual_ttest_df = pd.DataFrame([manual_ttest(v) for v in table1_vars])\nmanual_ttest_df\n\n\n\n\n\n\n\n\n\nVariable\nMean_Treatment\nMean_Control\nt-stat\n\n\n\n\n0\nmrm2\n13.012\n12.998000\n0.1195\n\n\n1\nhpa\n59.597\n58.959999\n0.9704\n\n\n2\nfreq\n8.035\n8.047000\n-0.1108\n\n\n3\nyears\n6.078\n6.136000\n-1.0909\n\n\n4\nyear5\n0.506\n0.514000\n-1.5627\n\n\n5\nfemale\n0.275\n0.283000\n-1.7727\n\n\n6\ncouple\n0.091\n0.093000\n-0.5888\n\n\n7\nredcty\n0.512\n0.507000\n0.9050\n\n\n8\nnonlit\n2.485\n2.453000\n1.7132\n\n\n9\ncases\n1.499\n1.502000\n-0.3428\n\n\n\n\n\n\n\nThis table replicates the balance check presented in Table 1 of Karlan & List (2007), using a manual t-test formula to compare pre-treatment characteristics across the treatment and control groups. The variables cover donor behavior (e.g., months since last donation, prior contributions), demographics (e.g., gender, race, household size), and political/geographic indicators (e.g., red state/county, legal involvement of the organization). Across all tested variables, the t-statistics remain small (generally below ±2), and none of the differences reach conventional levels of statistical significance. This provides strong evidence that the treatment assignment was effectively randomized and groups are balanced on observable characteristics. This confirms the reliability of the experiment design and supports the validity of later comparisons on donation behavior.\n\n\nShow code\n# from tabulate import tabulate\nimport statsmodels.formula.api as smf\n\nreg_results = []\nfor var in table1_vars:\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    reg_results.append([\n        var,\n        round(model.params[\"treatment\"], 4),\n        round(model.pvalues[\"treatment\"], 4),\n        round(model.rsquared, 4)\n    ])\n\n# headers = [\"Variable\", \"Treatment Coef\", \"p-value\", \"R-squared\"]\n# print(tabulate(reg_results, headers=headers, tablefmt=\"github\"))\nreg_df = pd.DataFrame(reg_results)\nreg_df  \n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\nmrm2\n0.0137\n0.9049\n0.0000\n\n\n1\nhpa\n0.6371\n0.3451\n0.0000\n\n\n2\nfreq\n-0.0120\n0.9117\n0.0000\n\n\n3\nyears\n-0.0575\n0.2700\n0.0000\n\n\n4\nyear5\n-0.0074\n0.1182\n0.0000\n\n\n5\nfemale\n-0.0075\n0.0787\n0.0001\n\n\n6\ncouple\n-0.0016\n0.5594\n0.0000\n\n\n7\nredcty\n0.0043\n0.3659\n0.0000\n\n\n8\nnonlit\n0.0318\n0.0888\n0.0001\n\n\n9\ncases\n-0.0037\n0.7333\n0.0000\n\n\n\n\n\n\n\nThis regression-based balance check complements the earlier t-tests by estimating the relationship between treatment assignment and baseline covariates. For each variable, we regress it on the treatment indicator and inspect the coefficient and p-value. All p-values are well above the 0.05 threshold, confirming that treatment status is not significantly associated with any pre-treatment variable. This is consistent with the design of a randomized controlled trial, further supporting the internal validity of the experiment. The results also match those from the manual t-tests, which is expected since the regression with a binary independent variable produces the same mean difference and inference as a t-test."
  },
  {
    "objectID": "blogs/hw1/index.html#experimental-results",
    "href": "blogs/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nShow code\nimport matplotlib.pyplot as plt\n\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\ndonation_rates.plot(kind=\"bar\", color=[\"green\", \"orange\"])\nplt.title(\"Proportion of People Who Donated\")\nplt.ylabel(\"Donation Rate\")\nplt.xticks(rotation=0)\nplt.ylim(0, 0.035)  \nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot displays the proportion of individuals who made a charitable contribution in the control and treatment groups. The donation rate is noticeably higher in the treatment group, indicating that the presence of a matching donation offer likely increased the likelihood of giving. This supports the hypothesis that matched donations can serve as an effective behavioral nudge in charitable fundraising.\n\n\nShow code\nfrom scipy.stats import ttest_ind\n\ntreated = df[df['treatment'] == 1]\ncontrol = df[df['treatment'] == 0]\n\nt_stat, p_val = ttest_ind(treated[\"gave\"], control[\"gave\"], equal_var=False)\n\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n\nt-statistic: 3.2095, p-value: 0.0013\n\n\n\n\nShow code\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\n\nfrom tabulate import tabulate\ntable = [[\"Coef\", model.params[\"treatment\"]],\n         [\"p-value\", model.pvalues[\"treatment\"]],\n         [\"R-squared\", model.rsquared]]\n# print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\ndf_table = pd.DataFrame(table)\ndf_table \n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nCoef\n0.004180\n\n\n1\np-value\n0.001927\n\n\n2\nR-squared\n0.000192\n\n\n\n\n\n\n\nBoth the t-test and linear regression show a statistically significant difference in donation behavior between the treatment and control groups. Individuals who received a letter mentioning a matching grant were more likely to donate than those who received a standard letter. This finding replicates Table 2a Panel A of Karlan & List (2007), and supports the hypothesis that matched donations act as a strong psychological motivator. From a behavioral standpoint, it suggests that people are more inclined to give when their contributions feel amplified — they may view their gift as having a greater impact, which encourages them to take action. This simple intervention—adding a match offer—significantly influenced behavior even though the individuals in both groups received similar messages aside from that detail.\n\n\nShow code\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom tabulate import tabulate\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=0)\n\nprobit_summary = [\n    [\"Coef\", probit_model.params[\"treatment\"]],\n    [\"p-value\", probit_model.pvalues[\"treatment\"]],\n    [\"Pseudo R-squared\", probit_model.prsquared]\n]\n#print(tabulate(probit_summary, headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\ndf_probit_summary = pd.DataFrame(probit_summary)\ndf_probit_summary\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nCoef\n0.086785\n\n\n1\np-value\n0.001852\n\n\n2\nPseudo R-squared\n0.000978\n\n\n\n\n\n\n\nThis probit regression estimates the effect of treatment assignment on the likelihood of making a donation, using a nonlinear model suitable for binary outcomes. The coefficient on the treatment variable is positive and statistically significant, indicating that individuals who received a matching gift message were more likely to donate. This result replicates the findings of Table 3, Column 1 in Karlan & List (2007), providing further evidence that matching donations increase participation. While the probit coefficient itself isn’t directly interpretable as a percentage change, its sign and significance confirm the effect observed in the linear model and t-test. Together, these consistent results across models support the conclusion that match framing can significantly increase donor engagement in charitable campaigns.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nShow code\nfrom scipy.stats import ttest_ind\n\nmatch_only = df[df[\"treatment\"] == 1]\n\nratio1 = match_only[match_only[\"ratio\"] == \"ratio1\"]\nratio2 = match_only[match_only[\"ratio\"] == \"ratio2\"]\nratio3 = match_only[match_only[\"ratio\"] == \"ratio3\"]\n\ntests = {\n    \"2:1 vs 1:1\": ttest_ind(ratio2[\"gave\"], ratio1[\"gave\"], equal_var=False),\n    \"3:1 vs 1:1\": ttest_ind(ratio3[\"gave\"], ratio1[\"gave\"], equal_var=False),\n    \"3:1 vs 2:1\": ttest_ind(ratio3[\"gave\"], ratio2[\"gave\"], equal_var=False),\n}\n\nimport pandas as pd\nttest_results = pd.DataFrame([\n    [k, round(v[0], 4), round(v[1], 4)]\n    for k, v in tests.items()\n], columns=[\"Comparison\", \"t-statistic\", \"p-value\"])\n\nttest_results\n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\n\n\n\n\n0\n2:1 vs 1:1\nNaN\nNaN\n\n\n1\n3:1 vs 1:1\nNaN\nNaN\n\n\n2\n3:1 vs 2:1\nNaN\nNaN\n\n\n\n\n\n\n\nThe t-tests above examine whether increasing the match ratio (from 1:1 to 2:1 or 3:1) significantly affects the probability of donating. The results show that the differences in donation rates are not statistically significant** at conventional levels. This finding aligns with the authors’ comment on page 8 of Karlan & List (2007), where they state that “the figures suggest that higher match ratios did not lead to significantly greater giving rates.” Despite intuitive expectations, offering a 2:1 or 3:1 match did not lead to more people donating compared to a 1:1 match. This suggests that the psychological impact of having any match available may be more important than the generosity of the multiplier itself.\n\n\nShow code\nimport statsmodels.formula.api as smf\nfrom tabulate import tabulate\n\nmatched = df[df[\"treatment\"] == 1]\n\nmodel = smf.ols(\"gave ~ C(ratio)\", data=matched).fit()\n\nsummary_table = [\n    [\"Intercept (ratio1)\", model.params[\"Intercept\"], model.pvalues[\"Intercept\"]],\n    [\"ratio2\", model.params.get(\"C(ratio)[T.ratio2]\", 0), model.pvalues.get(\"C(ratio)[T.ratio2]\", 1)],\n    [\"ratio3\", model.params.get(\"C(ratio)[T.ratio3]\", 0), model.pvalues.get(\"C(ratio)[T.ratio3]\", 1)],\n]\n#print(tabulate(summary_table, headers=[\"Term\", \"Coef\", \"p-value\"], tablefmt=\"github\"))\ndf_summary_table = pd.DataFrame(summary_table)\ndf_summary_table\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\nIntercept (ratio1)\n1.230440e+09\n0.912402\n\n\n1\nratio2\n0.000000e+00\n1.000000\n\n\n2\nratio3\n0.000000e+00\n1.000000\n\n\n\n\n\n\n\nThis regression estimates the donation probability under different match ratios, using ratio1 (1:1 match) as the baseline category. The intercept represents the average donation rate for the 1:1 group, while the coefficients for ratio2 and ratio3 represent the change in probability relative to that baseline. The results confirm that there are no statistically significant differences in donation rates when comparing 2:1 or 3:1 match ratios to the 1:1 ratio. These coefficients are close to zero and accompanied by large p-values, reinforcing the earlier t-test findings. In line with the authors’ interpretation, this suggests that increasing the match ratio beyond 1:1 does not further motivate giving** — the existence of a match alone may be the primary driver of behavioral change, rather than its size.\n\n\nShow code\ncoef_ratio1 = model.params[\"Intercept\"]\ncoef_ratio2 = model.params.get(\"C(ratio)[T.ratio2]\", 0)\ncoef_ratio3 = model.params.get(\"C(ratio)[T.ratio3]\", 0)\n\ndiff_2vs1 = coef_ratio2\ndiff_3vs2 = coef_ratio3 - coef_ratio2\n\nimport pandas as pd\nresponse_diff = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\"],\n    \"Difference in Fitted Rate\": [round(diff_2vs1, 5), round(diff_3vs2, 5)]\n})\n\nresponse_diff\n\n\n\n\n\n\n\n\n\nComparison\nDifference in Fitted Rate\n\n\n\n\n0\n2:1 vs 1:1\n0\n\n\n1\n3:1 vs 2:1\n0\n\n\n\n\n\n\n\nThis table shows the difference in predicted donation rates based on the fitted values from the regression model. - The difference between the 2:1 and 1:1 match groups is very small and not statistically significant. - The difference between the 3:1 and 2:1 groups is similarly negligible.\nThese findings provide further evidence that increasing the match ratio beyond 1:1 does not meaningfully increase the likelihood of donating. The presence of a match appears to matter, but its size does not — supporting the idea that the psychological nudge of a match is more about its existence than its magnitude. This aligns with the interpretation found in Karlan & List (2007), where the authors argue that the offer of any match (even 1:1) seems sufficient to trigger the intended behavioral response.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\nShow code\nfrom scipy.stats import ttest_ind\n\nt_stat, p_val = ttest_ind(treated[\"amount\"], control[\"amount\"], equal_var=False)\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n\nt-statistic: 1.9183, p-value: 0.0551\n\n\n\n\nShow code\nimport statsmodels.formula.api as smf\nfrom tabulate import tabulate\n\nmodel_amt = smf.ols(\"amount ~ treatment\", data=df).fit()\n\nsummary_table = [\n    [\"Intercept (Control)\", model_amt.params[\"Intercept\"], model_amt.pvalues[\"Intercept\"]],\n    [\"Treatment Coef\", model_amt.params[\"treatment\"], model_amt.pvalues[\"treatment\"]],\n    [\"R-squared\", model_amt.rsquared, \"\"]\n]\n# print(tabulate(summary_table, headers=[\"Term\", \"Coef\", \"p-value\"], tablefmt=\"github\"))\ndf_summary_table = pd.DataFrame(summary_table)\ndf_summary_table\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\nIntercept (Control)\n0.813268\n0.0\n\n\n1\nTreatment Coef\n0.153605\n0.06282\n\n\n2\nR-squared\n0.000069\n\n\n\n\n\n\n\n\nThis analysis tests whether the average donation amount differs between the treatment and control groups — regardless of whether someone donated or not (i.e., this includes all the zeros). Both the t-test and linear regression show that the mean donation amount is slightly higher in the treatment group, but the difference is small in magnitude and statistically weak. The low R-squared value also indicates that treatment status explains very little of the variance in donation amounts. This suggests that while matched donations may influence whether people give, they don’t strongly influence how much they give, at least when considering all individuals (including those who gave $0). To better understand donation behavior among actual donors, we’ll refine this analysis next by limiting it to individuals who did donate.\n\n\nShow code\ndonors_only = df[df[\"gave\"] == 1]\n\nmodel_conditional = smf.ols(\"amount ~ treatment\", data=donors_only).fit()\n\nsummary_table = [\n    [\"Intercept (Control)\", model_conditional.params[\"Intercept\"], model_conditional.pvalues[\"Intercept\"]],\n    [\"Treatment Coef\", model_conditional.params[\"treatment\"], model_conditional.pvalues[\"treatment\"]],\n    [\"R-squared\", model_conditional.rsquared, \"\"]\n]\n#print(tabulate(summary_table, headers=[\"Term\", \"Coef\", \"p-value\"], tablefmt=\"github\"))\ndf_summary_table = pd.DataFrame(summary_table)\ndf_summary_table\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\nIntercept (Control)\n45.540268\n0.0\n\n\n1\nTreatment Coef\n-1.668393\n0.561476\n\n\n2\nR-squared\n0.000327\n\n\n\n\n\n\n\n\nBy focusing only on those who actually donated, this regression estimates how much more (or less) people gave if they were in the treatment group compared to the control group. The estimated coefficient on treatment now reflects the difference in average gift size conditional on donating. The result shows that the treatment has very little effect on the amount donated among those who gave something — the coefficient is small and statistically insignificant. This finding supports the idea that the matching grant mainly affects the decision to donate, not the amount donated once that decision is made. In other words, treatment increases the extensive margin (whether to give), but not the intensive margin (how much to give). Regarding causal interpretation: since treatment was randomly assigned, and we’re conditioning on an outcome (gave), the estimate does not have a strict causal interpretation. It’s subject to selection bias — those who gave in treatment might be different in unobservable ways from those who gave in control. So we interpret this descriptively rather than causally.\n\n\nShow code\nimport matplotlib.pyplot as plt\n\ndonors_only = df[df[\"gave\"] == 1]\n\ntreat_donors = donors_only[donors_only[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = donors_only[donors_only[\"treatment\"] == 0][\"amount\"]\n\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Treatment plot\naxes[0].hist(treat_donors, bins=30, color='orange', edgecolor='black', alpha=0.8)\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f'Mean = {mean_treat:.2f}')\naxes[0].set_title(\"Treatment Group\")\naxes[0].set_xlabel(\"Donation Amount ($)\")\naxes[0].set_ylabel(\"Number of Donors\")\naxes[0].legend()\n\n# Control plot\naxes[1].hist(control_donors, bins=30, color='green', edgecolor='black', alpha=0.8)\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f'Mean = {mean_control:.2f}')\naxes[1].set_title(\"Control Group\")\naxes[1].set_xlabel(\"Donation Amount ($)\")\naxes[1].legend()\n\nfig.suptitle(\"Histogram of Donation Amounts (Among Donors Only)\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese histograms visualize the distribution of donation amounts among those who gave, separately for the treatment and control groups. The red vertical lines represent the sample average donation in each group. Below are the main observations: - Both distributions are highly right-skewed with many small donations and a few large outliers. - The average donation amount is slightly lower in the treatment group than in the control group. These observations align with earlier regression findings showing that while matched gifts increase the likelihood of donating, they do not increase the amount given by those who choose to donate. This suggests that matching gifts influence whether people give, but not how much they give, reinforcing the psychological interpretation that matched gifts are more about prompting action than amplifying generosity."
  },
  {
    "objectID": "blogs/hw1/index.html#simulation-experiment",
    "href": "blogs/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ttest_ind\n\np_control = 0.018\np_treat = 0.022\nn = 1000             \niterations = 10000   \n\nt_stats = []\n\nfor _ in range(iterations):\n    control_sample = np.random.binomial(1, p_control, n)\n    treat_sample = np.random.binomial(1, p_treat, n)\n    t_stat, _ = ttest_ind(treat_sample, control_sample, equal_var=False)\n    t_stats.append(t_stat)\n\nplt.figure(figsize=(10, 5))\nplt.hist(t_stats, bins=50, color='orange', edgecolor='black', density=True)\nplt.axvline(np.mean(t_stats), color='red', linestyle='--', label=f\"Mean t = {np.mean(t_stats):.3f}\")\nplt.axvline(1.96, color='gray', linestyle=':', label='95% Critical Value')\nplt.axvline(-1.96, color='gray', linestyle=':')\nplt.title(\"Simulated Distribution of t-Statistics\")\nplt.xlabel(\"t-statistic\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis simulation mimics running 10,000 randomized experiments comparing donation rates between a control group (with p = 0.018) and a treatment group (with p = 0.022), each with 1,000 individuals. I calculate the t-statistic for the difference in proportions in each simulated trial. The resulting histogram approximates the sampling distribution of the t-statistic under these parameters. The following are the key observations based on the simulation result: - The distribution is centered slightly above 0, reflecting the small true difference in means (0.004). - It approximates a normal distribution, which is expected from the Central Limit Theorem. - The proportion of simulated t-statistics exceeding ±1.96 shows how often a standard hypothesis test would reject the null of no difference at the 5% level.\nThis visually demonstrates how small but real differences can become detectable with sufficient sample size and repeated sampling — a core insight from the Law of Large Numbers and Central Limit Theorem.\n\nLaw of Large Numbers\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 10000\np_control = 0.018\np_treat = 0.022\n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treat, n)\n\ndiff = treatment - control\ncumulative_avg = np.cumsum(diff) / np.arange(1, n + 1)\ntrue_diff = p_treat - p_control\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, color='orange', lw=2, label=\"Cumulative Average\")\nplt.axhline(true_diff, color='red', linestyle='--', label=f\"True Diff = {true_diff:.4f}\")\nplt.title(\"Law of Large Numbers: Convergence of Mean Differences\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Difference (Treatment - Control)\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot illustrates how the cumulative average difference in donation outcomes between treatment and control groups converges to the true difference in means (0.004) as more data is accumulated. Each point on the blue line shows the average difference in outcomes after that many paired observations. As the sample size increases, the average stabilizes — a clear demonstration of the Law of Large Numbers in action. The dashed red line marks the true difference in the population, and we see that the sample-based average gets increasingly close to it. This reflects why larger experiments give more reliable, less variable estimates.\n\n\nCentral Limit Theorem\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np_control = 0.018\np_treat = 0.022\ntrue_diff = p_treat - p_control\nsample_sizes = [50, 200, 500, 1000]\niterations = 1000\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    for _ in range(iterations):\n        ctrl = np.random.binomial(1, p_control, n)\n        treat = np.random.binomial(1, p_treat, n)\n        mean_diffs.append(np.mean(treat) - np.mean(ctrl))\n    \n    axs[i].hist(mean_diffs, bins=30, color='orange', edgecolor='black')\n    axs[i].axvline(true_diff, color='red', linestyle='--', label=f\"True diff = {true_diff:.4f}\")\n    axs[i].axvline(0, color='black', linestyle=':', label=\"Zero\")\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].legend()\n    axs[i].set_xlabel(\"Sample Mean Differences\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].grid(True, linestyle='--', alpha=0.5)\n\nfig.suptitle(\"Central Limit Theorem: Distribution of Sample Mean Differences\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nEach histogram shows the distribution of sample mean differences between treatment and control groups from 1000 repeated experiments. The sample size increases from 50 to 1000 across the four plots. The following are the key takeaways: - With small samples (like ( n = 50 )), the distribution of average differences is wide and rough, with a high chance of extreme values. - As sample size increases, the distribution becomes smoother, more symmetric, and tightly clustered around the true difference(0.004). - This is a clear visual demonstration of the Central Limit Theorem: even though each individual donation is binary (0 or 1), the distribution of the sample mean becomes approximately normal for large ( n ). - Importantly, zero is not centered in the distribution, indicating that there is a real effect and that repeated experiments would often detect it."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, xc xmtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Analysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nJun 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]