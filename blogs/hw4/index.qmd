---
title: "Machine Learning"
author: "Lebin Sun"
date: June 11, 2025
---

_todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._


## 1a. K-Means
### Load and Prepare the Data

We start by loading the Palmer Penguins dataset and selecting only the two numeric features we'll use for clustering: **bill length** and **flipper length**. We also scale the data to have zero mean and unit variance.

```{python}
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load data and select features
df = pd.read_csv("palmer_penguins.csv")
df = df[['bill_length_mm', 'flipper_length_mm']].dropna()

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(df)
```

### Define Helper Functions

To build our own K-Means algorithm, we need a function to compute Euclidean distances between points and cluster centroids.

```{python}
# Helper: Euclidean distance between array of points and a centroid
def euclidean(a, b):
    return np.linalg.norm(a - b, axis=1)
```

### Implement K-Means from Scratch

Here's the main loop of the K-Means algorithm:
1. Randomly initialize `k` centroids.
2. Assign each point to the nearest centroid.
3. Recompute the centroids.
4. Repeat until convergence.

We also keep track of centroid history for visualization.

```{python}
def kmeans_custom(X, k=3, max_iters=10, seed=42):
    np.random.seed(seed)
    centroids = X[np.random.choice(len(X), size=k, replace=False)]
    history = [centroids.copy()]

    for _ in range(max_iters):
        distances = np.array([euclidean(X, c) for c in centroids])
        labels = np.argmin(distances, axis=0)
        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])
        
        history.append(new_centroids.copy())
        if np.allclose(new_centroids, centroids):
            break
        centroids = new_centroids
    
    return labels, centroids, history
```

### Run the Algorithm

Let’s run the custom K-Means on the penguins dataset with `k=3` and visualize the result.

```{python}
labels, final_centroids, history = kmeans_custom(X, k=3)
```

### Visualize Final Clustering

We plot the data points colored by cluster assignment, along with the final centroids.

```{python}
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)
plt.scatter(final_centroids[:, 0], final_centroids[:, 1], color='red', marker='X', s=200)
plt.xlabel("Bill Length (scaled)")
plt.ylabel("Flipper Length (scaled)")
plt.title("Custom K-Means Clustering (k=3)")
plt.grid(True)
plt.show()
```

The plot illustrates the result of applying a custom K-Means clustering algorithm with k=3 to the Palmer Penguins dataset, using bill length and flipper length as input features. Each point represents a penguin, colored by its assigned cluster, and the large red Xs indicate the final centroid of each cluster. The clustering reveals three distinct groups: one consisting of penguins with relatively shorter bills and flippers (left cluster), another with longer bills but shorter flippers (bottom-right), and a third with both longer bills and flippers (top-right). The clear separation and compactness of the clusters suggest that bill and flipper length are useful features for grouping the penguins, potentially reflecting differences in species or ecological niches. Since the features were standardized, the interpretation focuses on relative size rather than raw measurements.

### Evaluate the Optimal Number of Clusters

To assess the best value for \( k \), we compute two metrics:

- **Within-Cluster Sum of Squares (WCSS)**: Measures compactness; lower is better.
- **Silhouette Score**: Measures how well-separated the clusters are; ranges from -1 to 1, and higher is better.

We compute both for \( k = 2 \) to \( 7 \) using the built-in `KMeans` implementation from `sklearn`.

```{python}
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

wcss = []
silhouette_scores = []
k_values = range(2, 8)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    
    wcss.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X, labels))
```

### Plot the Metrics

We now visualize the WCSS and silhouette scores to identify the "best" number of clusters.

```{python}
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# WCSS (Elbow method)
ax[0].plot(k_values, wcss, marker='o')
ax[0].set_title('Elbow Method (WCSS)')
ax[0].set_xlabel('Number of clusters (k)')
ax[0].set_ylabel('WCSS')

# Silhouette Scores
ax[1].plot(k_values, silhouette_scores, marker='o', color='green')
ax[1].set_title('Silhouette Score')
ax[1].set_xlabel('Number of clusters (k)')
ax[1].set_ylabel('Score')

plt.tight_layout()
plt.show()
```

The Elbow Method plot displays the Within-Cluster Sum of Squares (WCSS) for different values of k ranging from 2 to 7. WCSS measures the compactness of the clusters, with lower values indicating tighter, more cohesive clusters. As the number of clusters increases, WCSS naturally decreases. However, the key insight comes from identifying the "elbow" point—where the rate of decrease sharply levels off. In this plot, the most noticeable elbow appears at k=3, suggesting that increasing the number of clusters beyond 3 results in only marginal improvements in cluster compactness. Therefore, k=3 is likely the optimal number of clusters, balancing simplicity with good fit.

## 1b. Latent-Class MNL

_todo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57._

_The data provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current "wide" format into a "long" format._

_todo: Fit the standard MNL model on these data.  Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes._

_todo: How many classes are suggested by the $BIC = -2*\ell_n  + k*log(n)$? (where $\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) The Bayesian-Schwarz Information Criterion [link](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate -- akin to the adjusted R-squared for the linear regression model. Note, that a **lower** BIC indicates a better model fit, accounting for the number of parameters in the model._

_todo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC._



## 2a. K Nearest Neighbors

_todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function._

```{python}
import numpy as np
import pandas as pd

# gen data
np.random.seed(42)
n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
x = np.column_stack((x1, x2))

# define a wiggly boundary
boundary = np.sin(4 * x1) + x1
y = np.where(x2 > boundary, 1, 0).astype(str)  # convert to string to mimic factor
dat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})

```

_todo: plot the data where the horizontal axis is `x1`, the vertical axis is `x2`, and the points are colored by the value of `y`.  You may optionally draw the wiggly boundary._

_todo: generate a test dataset with 100 points, using the same code as above but with a different seed._

_todo: implement KNN by hand.  Check you work with a built-in function -- eg, `class::knn()` or `caret::train(method="knn")` in R, or scikit-learn's `KNeighborsClassifier` in Python._

_todo: run your function for k=1,...,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?_ 



## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._






