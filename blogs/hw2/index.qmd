---
title: "Poisson Regression Examples"
author: "Lebin Sun"
date: May 7th, 2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

Now, let's first read in the Blueprinty data.
```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Read the data
blueprinty = pd.read_csv("blueprinty.csv")

# Quick look at the data
desc = blueprinty.drop(columns=["iscustomer"]).describe()
desc_rounded = desc.copy()
desc_rounded.loc["mean"] = desc_rounded.loc["mean"].round(2)
desc_rounded.loc["std"] = desc_rounded.loc["std"].round(2)
desc_rounded.loc[~desc_rounded.index.isin(["mean", "std"])] = desc_rounded.loc[~desc_rounded.index.isin(["mean", "std"])].round(0)
desc_rounded.style\
    .format("{:.2f}", subset=pd.IndexSlice["mean", :])\
    .format("{:.2f}", subset=pd.IndexSlice["std", :])\
    .format("{:.0f}", subset=pd.IndexSlice[["count", "min", "25%", "50%", "75%", "max"], :])
```
The above table displays the statistic summary of numerical variables in the dataset. 

Next, a histogram of number of patens distribution by customer status bellow compares differences between customer groups. 
```{python}
# Set up the plot style
sns.set(style="whitegrid")
means = blueprinty.groupby("iscustomer")["patents"].mean()
# Histogram of number of patents by customer status
plt.figure(figsize=(8, 5))
sns.histplot(data=blueprinty, x="patents", hue="iscustomer", multiple="dodge", binwidth=1, palette={0: "orange", 1: "green"})

plt.axvline(means[0], color="orange", linestyle="--", linewidth=2, label="Mean (Non-customer)")
plt.axvline(means[1], color="green", linestyle="--", linewidth=2, label="Mean (Customer)")

plt.title("Distribution of Patents by Customer Status")
plt.xlabel("Number of Patents")
plt.ylabel("Count")
plt.legend(title="Customer Status", labels=["Non-customer", "Customer"])
plt.tight_layout()
plt.show()
```
From the histogram and mean values, it appears that customers of Blueprinty's software tend to have more patents on average than non-customers. This may suggest that the software is associated with greater patenting success, but further modeling is necessary to control for confounding variables.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

After observing that Blueprinty customers might not be randomly selected, we explore whether there are systematic differences in region and firm age between customers and non-customers.

```{python}
# Bar plot of region distribution by customer status
blueprinty["iscustomer"] = blueprinty["iscustomer"].astype(str).astype(int)
plt.figure(figsize=(8, 4))
sns.countplot(data=blueprinty, x="region", hue="iscustomer", palette={0: "orange", 1: "green"})
plt.title("Region Distribution by Customer Status")
plt.xlabel("Region")
plt.ylabel("Count")
plt.legend(title="Customer", labels=["Non-customer", "Customer"])
plt.tight_layout()
plt.show()
```

The plot shows that customer adoption varies by region. For instance, the Northeast has a disproportionately high number of customers, while the Southwest and Northwest are dominated by non-customers. This implies that regional variation could confound any relationship between software usage and patenting success, so it should be accounted for in modeling.

```{python}
# Boxplot of age by customer status
plt.figure(figsize=(8, 4))
sns.boxplot(data=blueprinty, x="iscustomer", y="age", hue="iscustomer", palette={0: "orange", 1: "green"})
plt.title("Age Distribution by Customer Status")
plt.xlabel("Customer Status (0 = Non-customer, 1 = Customer)")
plt.ylabel("Age")
plt.tight_layout()
plt.show()
```

Customers appear to be slightly older than non-customers, with a higher median and a broader age distribution. While the difference is modest, it still points to the possibility that more mature firms are more likely to adopt Blueprinty's tools. Again, this reinforces the need to adjust for age when estimating treatment effects.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

The probability mass function of a Poisson distribution is:

$$
P(Y_i \mid \lambda) = \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Assuming independence across observations, the likelihood for a sample of \( n \) observations is:

$$
L(\lambda \mid Y_1, \dots, Y_n) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

And the log-likelihood function is:

$$
\log L(\lambda) = \sum_{i=1}^{n} \left[ -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right]
$$

Now translate the log-likelihood expression into Python code. The function below takes a proposed value of 
λ and a vector of observed patent counts Y, and returns the total log-likelihood. We use gammaln(Y + 1) instead of log(Y!) to ensure numerical stability and avoid issues with large factorials.

```{python}
import numpy as np
from scipy.special import gammaln

def poisson_loglikelihood(lambda_, Y):
    """Compute the Poisson log-likelihood for a given lambda and data Y"""
    if lambda_ <= 0:
        return -np.inf
    loglik = np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))
    return loglik
```

This function will allow us to evaluate the fit of any given λ to the data. In the next step, we’ll search for the value of λ that maximizes this log-likelihood — our Maximum Likelihood Estimate (MLE).

```{python}
# Define a range of lambda values to evaluate
lambda_vals = np.linspace(0.1, 10, 200)

# Compute log-likelihood for each lambda value
loglik_vals = [poisson_loglikelihood(l, blueprinty["patents"]) for l in lambda_vals]

# Plot the log-likelihood curve
plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, loglik_vals, color="purple")
plt.xlabel("Lambda (λ)")
plt.ylabel("Log-Likelihood")
plt.title("Poisson Log-Likelihood over a Range of λ")
plt.grid(True)
plt.tight_layout()
plt.show()
```

Let’s now find the Maximum Likelihood Estimate (MLE) for **lambda** by taking the derivative of the log-likelihood and setting it equal to zero:

\[
\frac{d}{d\lambda} \log L(\lambda) = \sum_{i=1}^{n} \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{1}{\lambda} \sum_{i=1}^{n} Y_i
\]

Setting this equal to zero:

\[
-n + \frac{1}{\lambda} \sum Y_i = 0 \quad \Rightarrow \quad \lambda = \frac{1}{n} \sum Y_i = \bar{Y}
\]

So the MLE for **lambda** is simply the sample mean of the observed counts.


And here’s a small Python code block to verify this numerically:

```{python}
lambda_mle = blueprinty["patents"].mean()
lambda_mle.round(3)
```

We can now find the Maximum Likelihood Estimate (MLE) for **lambda** numerically by maximizing the log-likelihood function. Since `scipy.optimize` performs minimization, we minimize the *negative* log-likelihood over a reasonable range of values for **lambda**.

```{python}
from scipy.optimize import minimize_scalar

result = minimize_scalar(
    lambda l: -poisson_loglikelihood(l, blueprinty["patents"]),
    bounds=(0.1, 10),
    method="bounded"
)

lambda_mle_opt = result.x
lambda_mle_opt.round(3)
```

The numerical estimate of lambda (MLE) is **3.685**, which matches the sample mean of the observed patent counts.

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

```{python}
import numpy as np
from scipy.special import gammaln

def poisson_regression_loglikelihood(beta, Y, X):
    beta = np.asarray(beta, dtype=float)
    eta = X @ beta
    lambda_ = np.exp(eta)
    loglik = np.sum(-lambda_ + Y * np.log(lambda_ + 1e-10) - gammaln(Y + 1))
    return loglik
```

We prepare the covariate matrix X so that it has the structure needed for Poisson regression: a constant column, numeric variables, and encoded categorical variables.

```{python}
import pandas as pd
import numpy as np

# Create dummy variables for region (drop one to avoid multicollinearity)
blueprinty_dummies = pd.get_dummies(blueprinty["region"], drop_first=True)

# Add age, age^2, customer, and intercept
blueprinty["age2"] = blueprinty["age"] ** 2
X = pd.concat([
    pd.Series(1, index=blueprinty.index, name="intercept"),
    blueprinty[["age", "age2", "iscustomer"]],
    blueprinty_dummies
], axis=1)

# Convert to numpy array
X_mat = X.values.astype(float)
Y = blueprinty["patents"].values.astype(float)
```

We use scipy.optimize.minimize with the BFGS method to find the MLEs of our regression coefficients.

```{python}
#| echo: true
#| warning: false
#| message: false

from scipy.optimize import minimize

# Define negative log-likelihood function
neg_loglik = lambda beta: -poisson_regression_loglikelihood(beta, Y, X_mat)

beta_init = np.full(X_mat.shape[1], -0.1)

# Minimize it
result = minimize(neg_loglik, beta_init, method="BFGS")
beta_hat = result.x
hessian_inv = result.hess_inv
```

The table below reports the estimated effect of each variable on the number of patents a firm receives, along with the standard error of each estimate.
```{python}
# Standard errors from the inverse Hessian
se = np.sqrt(np.diag(hessian_inv))
# Combine into a tidy table
coef_table = pd.DataFrame({
    "Variable": X.columns,
    "Estimate": beta_hat,
    "Std. Error": se
})
coef_table.round(4)
```

To confirm the accuracy of our hand-coded Poisson regression, we also estimate the model using `statsmodels.GLM` with the Poisson family. The table below presents the coefficient estimates, standard errors, z-scores, and confidence intervals.

```{python}
import statsmodels.api as sm

# Fit the Poisson regression using GLM
glm_model = sm.GLM(Y, X_mat, family=sm.families.Poisson())
glm_results = glm_model.fit()

# Display results in a clean table
glm_summary = glm_results.summary2().tables[1].reset_index().rename(columns={"index": "Variable"})
glm_summary.round(4)
```

The table above summarizes the results from our Poisson regression model using statsmodels. Here's what we observe:

- x1 (age): The coefficient is positive and highly significant (p<0.001), indicating that older firms tend to receive more patents, all else equal.

- x2 (age squared): The coefficient is small, negative, and also highly significant. This suggests a concave relationship — the positive effect of age diminishes at higher ages, meaning the relationship between age and patents is likely hump-shaped.

- x3 (iscustomer): The customer indicator is positive and significant. This implies that firms who use Blueprinty's software are associated with higher patent counts, controlling for other factors. This is consistent with the hypothesis that the software may help firms better navigate the patent process.

- x4 to x7 (region dummies): enderNone of the region dummy variables are statistically significant at conventional levels (p>0.05). This suggests that after controlling for age and customer status, the region of a firm does not meaningfully affect patent count in this sample.

- Intercept (const): The baseline level of patenting (for a non-customer firm with age zero in the reference region) is negative, as expected — but not of direct substantive interest.



Due to the nonlinear nature of the Poisson regression model, the estimated coefficients—particularly for binary variables like iscustomer—are not directly interpretable in terms of marginal effects on the outcome variable. That is, the coefficient on iscustomer does not represent a constant additive change in the expected number of patents.

To obtain a more interpretable estimate of the effect of using Blueprinty's software, we implement a counterfactual prediction approach. Specifically, we construct two hypothetical datasets:

- Scenario 1 (X₀): Each firm is treated as a non-customer (iscustomer = 0)

- Scenario 2 (X₁): Each firm is treated as a customer (iscustomer = 1)

All other covariates are held constant. We then use our fitted Poisson model to generate predicted patent counts for each firm under both scenarios. The average difference between the predicted outcomes under X₁ and X₀ represents the estimated effect of Blueprinty’s software on patenting activity.

Based on this approach, we estimate that firms using Blueprinty's software produce, on average, 0.79 more patents than they would have without the software, holding all else equal. This provides evidence consistent with the hypothesis that the software enhances patenting success.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._ 

### Load and Clean Data 

We begin by loading the AirBnB dataset, which is shown as below. The goal is to understand what factors are associated with a higher number of reviews, which we treat as a proxy for booking activity. 
```{python}
import pandas as pd

# Load the dataset
airbnb = pd.read_csv("airbnb.csv")
```

Below is a summary of the dataset's structure and variable distributions.
```{python}
airbnb.describe().T
```

Several variables in the dataset contain missing values. Notably, over 10,000 listings are missing review scores related to value, location, and cleanliness—likely because those listings received no reviews. Since our outcome of interest is the number of reviews, we can reasonably restrict our analysis to listings with at least one review. Additionally, we drop a small number of rows with missing data in key listing features such as bathrooms, bedrooms, and host listing date. Here’s the code to filter and clean accordingly:

```{python}
# Drop listings with zero reviews
airbnb_filtered = airbnb[airbnb["number_of_reviews"] > 0].copy()

# Drop rows with missing values in relevant predictors
cols_to_check = ["bathrooms", "bedrooms", "host_since",
                 "review_scores_cleanliness", "review_scores_location", "review_scores_value"]
airbnb_filtered = airbnb_filtered.dropna(subset=cols_to_check)
```

### Exploratory Data Analysis
To begin, we examine the distributions of key variables:

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

fig, axes = plt.subplots(2, 2, figsize=(12, 8))

sns.histplot(airbnb_filtered["number_of_reviews"], bins=50, color="green", kde=False, ax=axes[0, 0])
axes[0, 0].set_title("Number of Reviews")

sns.histplot(airbnb_filtered["price"], bins=50, color="green", kde=False, ax=axes[0, 1])
axes[0, 1].set_title("Price per Night")

sns.histplot(airbnb_filtered["review_scores_cleanliness"], bins=10, color="green", kde=False, ax=axes[1, 0])
axes[1, 0].set_title("Cleanliness Score")

sns.histplot(airbnb_filtered["review_scores_value"], bins=10, color="green", kde=False, ax=axes[1, 1])
axes[1, 1].set_title("Value Score")

plt.tight_layout()
plt.show()
```

Number of Reviews: Highly right-skewed. Most listings receive a modest number of reviews, but a small number receive over 100. This validates the use of a count-based model such as Poisson regression.

Price per Night: Also right-skewed, with the majority of listings priced under $500. A few outliers appear above $1,000.

Review Scores (Cleanliness & Value): Both are tightly clustered near the upper end of the scale (around 9–10), suggesting generally favorable customer feedback with limited variation.

These patterns support the idea that modeling review counts (as a proxy for bookings) using a Poisson regression is appropriate, provided we carefully control for influential predictors like price and review quality.

### Poisson Regression: Predicting Number of Reviews

We fit a Poisson regression model to predict the number of reviews (used as a proxy for bookings) based on a selection of listing characteristics.

```{python}
import statsmodels.api as sm
import pandas as pd

# Select predictors
predictors = [
    "price",
    "review_scores_cleanliness",
    "review_scores_location",
    "review_scores_value",
    "bedrooms",
    "bathrooms"
]

# Drop rows with missing data
airbnb_model_data = airbnb_filtered.dropna(subset=predictors + ["number_of_reviews"]).copy()

# Create design matrix
X = sm.add_constant(airbnb_model_data[predictors])
Y = airbnb_model_data["number_of_reviews"]

# Fit Poisson regression
poisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()

# Display summary
summary_table = poisson_model.summary2().tables[1].reset_index().rename(columns={"index": "Variable"})
summary_table.round(4)
```

- Intercept: Represents the expected log number of reviews when all predictors are at zero.
- Price: Small, negative, and statistically significant — more expensive listings receive slightly fewer reviews, on average.
- Review Scores – Cleanliness: Positive and highly significant. Cleanliness appears to drive guest engagement.
- Review Scores – Location & Value: Both are negative and significant, which may seem surprising. This could reflect multicollinearity or indicate that higher ratings correspond with fewer, but higher-quality bookings.

All coefficients are statistically significant at the 0.001 level, suggesting meaningful associations between listing characteristics and review frequency.

### Model Fit: Predicted vs. Actual Reviews
```{python}
airbnb_model_data["predicted_reviews"] = poisson_model.predict(X)

plt.figure(figsize=(8, 5))
sns.scatterplot(
    x=airbnb_model_data["number_of_reviews"],
    y=airbnb_model_data["predicted_reviews"],
    alpha=0.5, 
    color="green"
)
plt.xlabel("Actual Number of Reviews")
plt.ylabel("Predicted Number of Reviews")
plt.title("Predicted vs. Actual Number of Reviews (Poisson Model)")
plt.grid(True)
plt.tight_layout()
plt.show()
```
Comparing predicted review counts from the Poisson model to the actual observed counts for each listing, the plot shows a strong positive relationship between actual and predicted values, especially for listings with fewer than 100 reviews. There is some deviation for listings with very high review counts, suggesting possible overdispersion—a common issue in count data where variance exceeds the mean. Despite this, the model captures the general trend well and provides a solid baseline for understanding how listing characteristics relate to booking activity.

